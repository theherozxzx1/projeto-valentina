{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9023aa15",
   "metadata": {},
   "source": [
    "# üéØ Valentina Facial LoRA Trainer - Google Colab Edition\n",
    "\n",
    "Este notebook treina uma **LoRA de identidade facial** para a Valentina usando FLUX.1-dev + dataset otimizado no Google Colab.\n",
    "\n",
    "## üéØ Foco: Identidade Visual (N√ÉO NSFW)\n",
    "- **Objetivo**: Gerar a Valentina com m√°xima consist√™ncia facial\n",
    "- **Dataset**: Imagens da pasta `valentina_identity_4lora_dataset_flux`\n",
    "- **Caracter√≠sticas**: 25 anos, rosto oval, olhos amendoados, sem tatuagens\n",
    "- **Trigger Word**: `vltna woman`\n",
    "\n",
    "## üìã Stack Otimizada para Identidade:\n",
    "- **Base**: FLUX.1-dev (m√°xima qualidade facial)\n",
    "- **Dataset**: 18 imagens com seeds sequenciais (m√°xima consist√™ncia)\n",
    "- **Par√¢metros**: Conservadores para preservar caracter√≠sticas faciais\n",
    "- **Output**: LoRA facial da Valentina para uso local com mflux\n",
    "\n",
    "‚ö†Ô∏è **Importante**: Execute as c√©lulas em ordem e aguarde a conclus√£o de cada etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configura√ß√µes Principais ---\n",
    "BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\" # Modelo base oficial para identidade\n",
    "DATASET_ZIP_FILENAME = \"valentina_identity_4lora_dataset_flux.zip\" # Dataset de identidade gerado\n",
    "\n",
    "# --- Caminhos no Ambiente Colab ---\n",
    "COLAB_MODELS_PATH = \"/content/models\"\n",
    "COLAB_DATASET_PATH = \"/content/dataset\"\n",
    "COLAB_OUTPUT_PATH = \"/content/output_lora\"\n",
    "COLAB_LOGS_PATH = \"/content/logs\"\n",
    "\n",
    "# --- Par√¢metros de Treinamento da LoRA de Identidade ---\n",
    "INSTANCE_PROMPT = \"a photo of vltna woman\" # Token √∫nico para Valentina\n",
    "CLASS_PROMPT = \"a photo of a woman\" # Prompt de classe para regulariza√ß√£o\n",
    "\n",
    "# Configura√ß√µes de resolu√ß√£o e qualidade (baseado no dataset gerado)\n",
    "RESOLUTION = 1024  # Mesma resolu√ß√£o do dataset\n",
    "CENTER_CROP = True\n",
    "RANDOM_FLIP = False # NUNCA flipar para preservar caracter√≠sticas faciais\n",
    "\n",
    "# Batch sizes otimizados para identidade (conservadores)\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 4 # Batch efetivo menor para preservar detalhes\n",
    "\n",
    "# Learning rates CONSERVADORES para preservar identidade facial\n",
    "LEARNING_RATE = 5e-5 # Reduzido para preservar caracter√≠sticas faciais\n",
    "UNET_LR = 5e-5\n",
    "TEXT_ENCODER_LR = 3e-6 # Muito menor para text encoder\n",
    "\n",
    "# Scheduler e warmup otimizados para identidade\n",
    "LR_SCHEDULER = \"cosine_with_restarts\"\n",
    "LR_WARMUP_STEPS = 50 # Reduzido para dataset menor\n",
    "LR_NUM_CYCLES = 1\n",
    "\n",
    "# Steps de treinamento (calculado para dataset de identidade: ~80-120 steps por imagem)\n",
    "MAX_TRAIN_STEPS = 1500 # Para 18 imagens = ~83 steps/imagem (conservador)\n",
    "SAVE_STEPS = 300\n",
    "VALIDATION_EPOCHS = 3 # Reduzido para dataset menor\n",
    "\n",
    "# Arquitetura LoRA otimizada para IDENTIDADE FACIAL\n",
    "LORA_RANK = 64 # Rank menor para preservar identidade (era 128)\n",
    "LORA_ALPHA = 32 # Alpha = rank/2 para estabilidade\n",
    "LORA_DROPOUT = 0.05 # Dropout menor para melhor aprendizado\n",
    "\n",
    "# Configura√ß√µes de precis√£o e otimiza√ß√£o\n",
    "MIXED_PRECISION = \"bf16\" # Melhor qualidade se suportado\n",
    "USE_8BIT_ADAM = True\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_WEIGHT_DECAY = 0.01\n",
    "ADAM_EPSILON = 1e-8\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Memory optimization\n",
    "GRADIENT_CHECKPOINTING = True\n",
    "ENABLE_XFORMERS = True\n",
    "USE_CPU_OFFLOAD = False # Manter na GPU para velocidade\n",
    "\n",
    "# Regulariza√ß√£o para IDENTIDADE FACIAL\n",
    "PRIOR_LOSS_WEIGHT = 1.0\n",
    "SNR_GAMMA = 5.0 # Para melhor qualidade com ru√≠do\n",
    "\n",
    "# Checkpointing conservador\n",
    "CHECKPOINTING_STEPS = 300\n",
    "CHECKPOINTS_TOTAL_LIMIT = 3\n",
    "RESUME_FROM_CHECKPOINT = None\n",
    "\n",
    "# Seeds para reprodutibilidade (alinhado com dataset)\n",
    "SEED = 42 # Mesmo seed base do dataset\n",
    "\n",
    "# Configura√ß√µes espec√≠ficas para IDENTIDADE (n√£o NSFW)\n",
    "USE_MIDJOURNEY_LORA = False # Desabilitado - foco em identidade pura\n",
    "VALIDATION_PROMPT = \"a photo of vltna woman, professional portrait photography\" # Prompt de valida√ß√£o para identidade\n",
    "\n",
    "print(\"‚öôÔ∏è Configura√ß√µes otimizadas para IDENTIDADE FACIAL carregadas:\")\n",
    "print(f\"üìä Steps totais: {MAX_TRAIN_STEPS}\")\n",
    "print(f\"üéØ Batch efetivo: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"üß† LoRA Rank: {LORA_RANK} (reduzido para preservar identidade)\")\n",
    "print(f\"üé® Base Model: {BASE_MODEL_ID}\")\n",
    "print(f\"üíº Dataset: {DATASET_ZIP_FILENAME}\")\n",
    "print(f\"üé≠ Trigger: '{INSTANCE_PROMPT}'\")\n",
    "print(f\"üî¨ Foco: IDENTIDADE FACIAL (n√£o NSFW)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80027a40",
   "metadata": {},
   "source": [
    "## C√©lula 2: Setup do Ambiente Colab (Depend√™ncias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09fe240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ INSTALA√á√ÉO OTIMIZADA DE DEPEND√äNCIAS VIA UV\n",
    "print(\"üîß Instalando depend√™ncias usando uv para maior robustez...\")\n",
    "\n",
    "# 1. Instalar uv\n",
    "print(\"‚öôÔ∏è Instalando uv...\")\n",
    "!pip install -q uv\n",
    "print(\"‚úÖ uv instalado.\")\n",
    "\n",
    "# 2. Definir o conte√∫do do requirements.txt baseado no pyproject.toml do mflux\n",
    "requirements_content = \"\"\"\\\n",
    "torch>=2.1.0,<2.4.0\n",
    "torchvision>=0.16.0,<0.19.0\n",
    "torchaudio>=2.1.0,<2.4.0\n",
    "diffusers[torch]>=0.27.0,<1.0\n",
    "transformers>=4.44.0,<5.0\n",
    "accelerate>=0.32.0,<1.0\n",
    "safetensors>=0.4.0,<0.5.0\n",
    "xformers>=0.0.27,<0.0.29\n",
    "pillow>=10.0.0,<11.0.0\n",
    "opencv-python>=4.10.0,<5.0\n",
    "huggingface-hub>=0.24.5,<1.0\n",
    "sentencepiece>=0.2.0,<0.3.0\n",
    "tokenizers>=0.19.0,<0.20.0\n",
    "protobuf>=5.27.0,<6.0.0\n",
    "numpy>=2.0.0,<3.0.0\n",
    "requests>=2.32.0,<3.0.0\n",
    "scipy>=1.14.0,<2.0.0\n",
    "matplotlib>=3.9.0,<4.0.0\n",
    "omegaconf>=2.3.0,<3.0.0\n",
    "einops>=0.8.0,<0.9.0\n",
    "invisible-watermark>=0.2.0,<0.3.0\n",
    "compel>=2.0.0,<3.0.0\n",
    "wandb>=0.17.0,<0.18.0\n",
    "peft>=0.12.0,<0.13.0\n",
    "bitsandbytes>=0.43.0,<0.44.0\n",
    "gradio>=4.39.0,<5.0.0\n",
    "albumentations>=1.4.0,<2.0.0\n",
    "imageio>=2.34.0,<3.0.0\n",
    "scikit-image>=0.24.0,<0.25.0\n",
    "tqdm>=4.66.0,<5.0.0\n",
    "ftfy>=6.2.0,<7.0.0\n",
    "tensorboard>=2.16.0,<3.0.0\n",
    "easydict>=1.13.0,<2.0.0\n",
    "clean-fid==0.1.35\n",
    "torchmetrics>=1.4.0,<2.0.0\n",
    "kornia>=0.7.0,<0.8.0\n",
    "lpips>=0.1.4,<0.2.0\n",
    "controlnet_aux>=0.0.7,<0.0.8\n",
    "segment-anything>=1.0.0,<2.0.0\n",
    "rembg[gpu]>=2.0.56,<3.0.0\n",
    "moviepy>=1.0.3,<2.0.0\n",
    "typer>=0.12.0,<0.13.0\n",
    "rich>=13.7.0,<14.0.0\n",
    "shellingham>=1.5.0,<2.0.0\n",
    "\"\"\"\n",
    "\n",
    "# 3. Criar o arquivo requirements.txt no ambiente do Colab\n",
    "requirements_file_path = \"/content/valentina_flux_requirements.txt\"\n",
    "with open(requirements_file_path, \"w\") as f:\n",
    "    f.write(requirements_content)\n",
    "print(f\"üìÑ {requirements_file_path} criado com depend√™ncias do mflux.\")\n",
    "\n",
    "# 4. Instalar depend√™ncias usando uv pip install\n",
    "print(f\"üöÄ Instalando depend√™ncias de {requirements_file_path} com uv...\")\n",
    "print(\"üïí Isso pode levar alguns minutos...\")\n",
    "!uv pip install -q -r {requirements_file_path} --extra-index-url https://download.pytorch.org/whl/cu121 --index-strategy unsafe-best-match\n",
    "\n",
    "print(\"‚úÖ Depend√™ncias instaladas com sucesso usando uv!\")\n",
    "print(\"üéØ Ambiente otimizado para treinamento de LoRA FLUX\")\n",
    "\n",
    "# Verifica√ß√£o final de CUDA e PyTorch\n",
    "import torch\n",
    "print(f\"üî• PyTorch: {torch.__version__}\")\n",
    "print(f\"üéÆ CUDA dispon√≠vel: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéØ GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"üíæ VRAM total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "    print(f\"üßπ VRAM livre: {torch.cuda.memory_allocated() / 1024**3:.1f}GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA n√£o detectado - usando CPU (MUITO LENTO)\")\n",
    "\n",
    "# Limpeza de mem√≥ria inicial\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print(\"‚úÖ Verifica√ß√£o de ambiente conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d8568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñ•Ô∏è Verifica√ß√£o e Otimiza√ß√£o da GPU\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"üîç Verificando configura√ß√£o da GPU...\")\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits\n",
    "\n",
    "# Detectar capacidades da GPU\n",
    "gpu_name = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=name\", \"--format=csv,noheader\"]).decode().strip()\n",
    "print(f\"üì± GPU detectada: {gpu_name}\")\n",
    "\n",
    "# Ajustar configura√ß√µes baseado na GPU\n",
    "if \"T4\" in gpu_name:\n",
    "    print(\"üîß Otimiza√ß√µes para Tesla T4\")\n",
    "    MIXED_PRECISION = \"fp16\"  # T4 funciona melhor com fp16\n",
    "    TRAIN_BATCH_SIZE = 1\n",
    "    GRADIENT_ACCUMULATION_STEPS = 6\n",
    "elif \"V100\" in gpu_name:\n",
    "    print(\"üîß Otimiza√ß√µes para V100\")\n",
    "    MIXED_PRECISION = \"fp16\"\n",
    "    TRAIN_BATCH_SIZE = 1\n",
    "    GRADIENT_ACCUMULATION_STEPS = 8\n",
    "elif \"A100\" in gpu_name or \"H100\" in gpu_name:\n",
    "    print(\"üîß Otimiza√ß√µes para GPU high-end\")\n",
    "    MIXED_PRECISION = \"bf16\"  # Melhor qualidade\n",
    "    TRAIN_BATCH_SIZE = 2\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "else:\n",
    "    print(\"üîß Configura√ß√µes padr√£o\")\n",
    "    MIXED_PRECISION = \"fp16\"\n",
    "\n",
    "print(f\"‚úÖ Configura√ß√µes ajustadas: {MIXED_PRECISION}, batch={TRAIN_BATCH_SIZE}\")\n",
    "\n",
    "# Limpar cache da GPU\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"üßπ Cache da GPU limpo. Mem√≥ria livre: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "\n",
    "print(\"Instalando depend√™ncias... Por favor, aguarde.\")\n",
    "!pip install -q diffusers transformers accelerate bitsandbytes safetensors peft xformers huggingface_hub torch torchvision torchaudio --upgrade\n",
    "\n",
    "print(\"Depend√™ncias instaladas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "print(\"Por favor, fa√ßa login na sua conta Hugging Face para baixar os modelos.\")\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b111f",
   "metadata": {},
   "source": [
    "## C√©lula 4: Cria√ß√£o da Estrutura de Diret√≥rios no Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c01b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Cria√ß√£o da Estrutura de Diret√≥rios\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Criar todos os diret√≥rios necess√°rios\n",
    "directories = [\n",
    "    COLAB_MODELS_PATH,\n",
    "    COLAB_DATASET_PATH,\n",
    "    COLAB_OUTPUT_PATH,\n",
    "    COLAB_LOGS_PATH,\n",
    "    f\"{COLAB_DATASET_PATH}/instance_images\",\n",
    "    f\"{COLAB_DATASET_PATH}/class_images\",\n",
    "    f\"{COLAB_OUTPUT_PATH}/checkpoints\",\n",
    "    f\"{COLAB_OUTPUT_PATH}/samples\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"üìÇ Criado: {directory}\")\n",
    "\n",
    "print(\"\\n‚úÖ Estrutura de diret√≥rios criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69f9df",
   "metadata": {},
   "source": [
    "## C√©lula 5: Upload e Prepara√ß√£o do Dataset de Identidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d00b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì§ Upload e Prepara√ß√£o do Dataset de Identidade\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "print(\"üì• === UPLOAD DO DATASET DE IDENTIDADE ===\")\n",
    "print(f\"Por favor, fa√ßa upload do arquivo: {DATASET_ZIP_FILENAME}\")\n",
    "print(\"üß¨ Dataset gerado pelo valentina_dataset_generator_colab.ipynb\")\n",
    "print(\"üìä Cont√©m 18 imagens com seeds sequenciais para m√°xima consist√™ncia facial\")\n",
    "uploaded_dataset = files.upload()\n",
    "\n",
    "if DATASET_ZIP_FILENAME in uploaded_dataset:\n",
    "    print(f\"üì¶ Extraindo {DATASET_ZIP_FILENAME}...\")\n",
    "    with zipfile.ZipFile(DATASET_ZIP_FILENAME, 'r') as zip_ref:\n",
    "        zip_ref.extractall(COLAB_DATASET_PATH)\n",
    "    \n",
    "    # Encontrar as imagens extra√≠das\n",
    "    instance_images_path = f\"{COLAB_DATASET_PATH}/instance_images\"\n",
    "    \n",
    "    # Verificar estrutura do dataset\n",
    "    extracted_files = []\n",
    "    for root, dirs, files in os.walk(COLAB_DATASET_PATH):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                extracted_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"üñºÔ∏è Encontradas {len(extracted_files)} imagens de identidade\")\n",
    "    \n",
    "    # Mover imagens para pasta de inst√¢ncia se necess√°rio\n",
    "    if not os.path.exists(instance_images_path):\n",
    "        os.makedirs(instance_images_path, exist_ok=True)\n",
    "    \n",
    "    processed_count = 0\n",
    "    for i, img_path in enumerate(extracted_files):\n",
    "        new_path = f\"{instance_images_path}/valentina_{i:03d}.png\"\n",
    "        \n",
    "        # Verificar se j√° n√£o est√° na pasta correta\n",
    "        if os.path.dirname(img_path) == instance_images_path:\n",
    "            continue\n",
    "            \n",
    "        # Converter e redimensionar se necess√°rio\n",
    "        with Image.open(img_path) as img:\n",
    "            # Converter para RGB se necess√°rio\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Verificar se j√° est√° no tamanho correto\n",
    "            if img.size == (RESOLUTION, RESOLUTION):\n",
    "                # Salvar direto se j√° est√° no tamanho certo\n",
    "                img.save(new_path, 'PNG', quality=95)\n",
    "            else:\n",
    "                # Redimensionar mantendo aspect ratio\n",
    "                img.thumbnail((RESOLUTION, RESOLUTION), Image.Resampling.LANCZOS)\n",
    "                \n",
    "                # Criar imagem quadrada com padding\n",
    "                new_img = Image.new('RGB', (RESOLUTION, RESOLUTION), (255, 255, 255))\n",
    "                paste_x = (RESOLUTION - img.width) // 2\n",
    "                paste_y = (RESOLUTION - img.height) // 2\n",
    "                new_img.paste(img, (paste_x, paste_y))\n",
    "                \n",
    "                new_img.save(new_path, 'PNG', quality=95)\n",
    "        \n",
    "        processed_count += 1\n",
    "        print(f\"‚úÖ Processada: {os.path.basename(img_path)} -> {os.path.basename(new_path)}\")\n",
    "    \n",
    "    print(f\"\\nüìä Dataset de identidade preparado: {len(extracted_files)} imagens\")\n",
    "    print(f\"üéØ Imagens processadas: {processed_count}\")\n",
    "    !ls -la {instance_images_path}\n",
    "    \n",
    "    # Verificar se existe metadata do dataset\n",
    "    metadata_path = f\"{COLAB_DATASET_PATH}/dataset_metadata.json\"\n",
    "    if os.path.exists(metadata_path):\n",
    "        print(f\"üìã Metadata do dataset encontrado: {metadata_path}\")\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            import json\n",
    "            metadata = json.load(f)\n",
    "            if isinstance(metadata, list) and len(metadata) > 0:\n",
    "                first_meta = metadata[0]\n",
    "                print(f\"üß¨ Configura√ß√£o do dataset:\")\n",
    "                print(f\"   ‚Ä¢ Arquitetura: {first_meta.get('architecture', 'N/A')}\")\n",
    "                print(f\"   ‚Ä¢ Foco: {first_meta.get('optimization_focus', 'N/A')}\")\n",
    "                print(f\"   ‚Ä¢ Base Model: {first_meta.get('base_model', 'N/A')}\")\n",
    "                print(f\"   ‚Ä¢ Seeds: {metadata[0].get('seed', 'N/A')} a {metadata[-1].get('seed', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"‚ùå ERRO: {DATASET_ZIP_FILENAME} n√£o encontrado!\")\n",
    "    print(\"üîç Certifique-se de que:\")\n",
    "    print(\"1. Gerou o dataset usando valentina_dataset_generator_colab.ipynb\")\n",
    "    print(\"2. Baixou o arquivo valentina_identity_4lora_dataset_flux.zip\")\n",
    "    print(\"3. Est√° fazendo upload do arquivo correto\")\n",
    "    raise FileNotFoundError(\"Dataset de identidade n√£o foi enviado\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset de identidade preparado com sucesso!\")\n",
    "print(\"üß¨ Pronto para treinar LoRA focada em identidade facial\")\n",
    "print(f\"üé≠ Trigger word que ser√° treinada: '{INSTANCE_PROMPT}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249e809",
   "metadata": {},
   "source": [
    "## C√©lula 6: Prepara√ß√£o do Modelo Base para Treinamento de Identidade\n",
    "Carrega o modelo FLUX.1-dev oficial e o prepara para treinamento da LoRA de identidade facial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üöÄ Iniciando prepara√ß√£o do modelo base para identidade...\")\n",
    "\n",
    "# === VALIDA√á√ÉO LOCAL DO DATASET (antes do upload) ===\n",
    "# üîç Verifica√ß√£o se o dataset foi criado corretamente ANTES do upload\n",
    "local_dataset_path = Path(\"valentina_identity_4lora_dataset_flux\")\n",
    "if local_dataset_path.exists():\n",
    "    local_images = list(local_dataset_path.glob(\"*.png\")) + list(local_dataset_path.glob(\"*.jpg\"))\n",
    "    if len(local_images) >= 15:\n",
    "        print(f\"‚úÖ Dataset local v√°lido: {len(local_images)} imagens encontradas\")\n",
    "        print(\"üì¶ Pronto para fazer upload do dataset para o Colab\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Dataset incompleto: apenas {len(local_images)} imagens (m√≠nimo: 15)\")\n",
    "        print(\"üîÑ Execute novamente o valentina_dataset_generator_colab.ipynb\")\n",
    "else:\n",
    "    print(\"‚ùå Pasta valentina_identity_4lora_dataset_flux n√£o encontrada\")\n",
    "    print(\"üîÑ Execute o valentina_dataset_generator_colab.ipynb primeiro\")\n",
    "\n",
    "# Configurar dtype baseado na GPU\n",
    "if torch.cuda.is_available():\n",
    "    if torch.cuda.get_device_capability()[0] >= 8:  # Ampere+\n",
    "        model_dtype = torch.bfloat16\n",
    "        print(\"üíé Usando bfloat16 (GPU Ampere+)\")\n",
    "    else:\n",
    "        model_dtype = torch.float16\n",
    "        print(\"‚ö° Usando float16 (GPU older)\")\n",
    "else:\n",
    "    model_dtype = torch.float32\n",
    "    print(\"üêå Usando float32 (CPU)\")\n",
    "\n",
    "# Para o treinamento, o mais simples √© usar o modelo ID diretamente\n",
    "# O script de treinamento ir√° carregar o modelo automaticamente\n",
    "model_path = BASE_MODEL_ID\n",
    "print(f\"üéØ Modelo configurado para treinamento: {model_path}\")\n",
    "print(\"üß¨ Foco: Treinamento de identidade facial pura\")\n",
    "\n",
    "# Verificar disponibilidade do modelo\n",
    "print(f\"‚úÖ Modelo base preparado: {BASE_MODEL_ID}\")\n",
    "print(\"üìù O script de treinamento carregar√° o modelo automaticamente\")\n",
    "\n",
    "# Alternativamente, se precisar baixar o modelo localmente:\n",
    "try:\n",
    "    print(f\"\\nüì¶ Verificando disponibilidade do modelo: {BASE_MODEL_ID}\")\n",
    "    \n",
    "    # Teste r√°pido de carregamento para verificar acesso\n",
    "    pipeline = FluxPipeline.from_pretrained(\n",
    "        BASE_MODEL_ID,\n",
    "        torch_dtype=model_dtype,\n",
    "        use_safetensors=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Modelo acess√≠vel e compat√≠vel!\")\n",
    "    \n",
    "    # Salvar localmente se necess√°rio\n",
    "    model_save_path = f\"{COLAB_MODELS_PATH}/flux_base_for_identity\"\n",
    "    print(f\"üíæ Salvando modelo base em: {model_save_path}\")\n",
    "    \n",
    "    pipeline.save_pretrained(\n",
    "        model_save_path,\n",
    "        safe_serialization=True\n",
    "    )\n",
    "    \n",
    "    # Atualizar caminho para apontar para o modelo local\n",
    "    model_path = model_save_path\n",
    "    print(f\"üéØ Modelo salvo localmente: {model_path}\")\n",
    "    \n",
    "    # Verificar componentes do modelo\n",
    "    print(\"\\nüîç Componentes do modelo verificados:\")\n",
    "    if hasattr(pipeline, 'transformer'):\n",
    "        print(\"   ‚úÖ Transformer (componente principal)\")\n",
    "    if hasattr(pipeline, 'text_encoder'):\n",
    "        print(\"   ‚úÖ Text Encoder\") \n",
    "    if hasattr(pipeline, 'text_encoder_2'):\n",
    "        print(\"   ‚úÖ Text Encoder 2\")\n",
    "    if hasattr(pipeline, 'vae'):\n",
    "        print(\"   ‚úÖ VAE\")\n",
    "    \n",
    "    print(f\"\\nüéØ Modelo preparado: {model_path}\")\n",
    "    print(\"üß¨ Foco: Treinamento de identidade facial pura (sem LoRAs adicionais)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è N√£o foi poss√≠vel baixar o modelo localmente: {e}\")\n",
    "    print(\"üîÑ Usando modelo ID diretamente para o treinamento\")\n",
    "    model_path = BASE_MODEL_ID\n",
    "    print(f\"üéØ Modelo para treinamento: {model_path}\")\n",
    "\n",
    "finally:\n",
    "    # Limpar mem√≥ria\n",
    "    if 'pipeline' in locals():\n",
    "        del pipeline\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"üßπ Mem√≥ria limpa ap√≥s verifica√ß√£o do modelo\")\n",
    "\n",
    "print(f\"\\n‚úÖ Prepara√ß√£o conclu√≠da!\")\n",
    "print(f\"üé≠ Modelo pronto para treinamento de identidade: {model_path}\")\n",
    "print(\"üß¨ Pr√≥ximo: Iniciar treinamento da LoRA de identidade facial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd0a718",
   "metadata": {},
   "source": [
    "## C√©lula 7: Treinamento da LoRA de Identidade Facial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ C√âLULA 7: TREINAMENTO FLUX LORA DE IDENTIDADE FACIAL\n",
    "# Sistema completo otimizado para Google Colab\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import T5EncoderModel, T5Tokenizer, CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import FluxPipeline, FluxTransformer2DModel\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor2_0\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.optimization import get_scheduler\n",
    "import safetensors.torch as st\n",
    "\n",
    "print(\"üöÄ INICIANDO TREINAMENTO FLUX LORA DE IDENTIDADE FACIAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configurar seeds para reprodutibilidade\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# Verifica√ß√µes iniciais\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "\n",
    "print(f\"üì± Device: {device}\")\n",
    "print(f\"üî¢ Dtype: {dtype}\")\n",
    "print(f\"üíæ VRAM dispon√≠vel: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "\n",
    "# Dataset personalizado para FLUX\n",
    "class FluxLoRADataset(Dataset):\n",
    "    def __init__(self, dataset_path, instance_prompt, class_prompt, tokenizer_1, tokenizer_2, size=1024):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self.class_prompt = class_prompt\n",
    "        self.tokenizer_1 = tokenizer_1\n",
    "        self.tokenizer_2 = tokenizer_2\n",
    "        self.size = size\n",
    "        \n",
    "        # Encontrar imagens de inst√¢ncia\n",
    "        instance_path = self.dataset_path / \"instance_images\"\n",
    "        if not instance_path.exists():\n",
    "            instance_path = self.dataset_path\n",
    "        \n",
    "        self.instance_images = []\n",
    "        for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\"]:\n",
    "            self.instance_images.extend(list(instance_path.glob(ext)))\n",
    "        \n",
    "        print(f\"üìä Imagens de inst√¢ncia encontradas: {len(self.instance_images)}\")\n",
    "        \n",
    "        if len(self.instance_images) == 0:\n",
    "            raise ValueError(\"Nenhuma imagem de inst√¢ncia encontrada!\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instance_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Carregar e processar imagem\n",
    "        image_path = self.instance_images[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Redimensionar mantendo aspect ratio\n",
    "        image = image.resize((self.size, self.size), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Converter para tensor\n",
    "        image = torch.from_numpy(np.array(image)).float() / 255.0\n",
    "        image = image.permute(2, 0, 1)  # HWC -> CHW\n",
    "        \n",
    "        # Normalizar para [-1, 1]\n",
    "        image = (image - 0.5) * 2.0\n",
    "        \n",
    "        # Tokenizar prompts\n",
    "        instance_tokens_1 = self.tokenizer_1(\n",
    "            self.instance_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        instance_tokens_2 = self.tokenizer_2(\n",
    "            self.instance_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        class_tokens_1 = self.tokenizer_1(\n",
    "            self.class_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        class_tokens_2 = self.tokenizer_2(\n",
    "            self.class_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"instance_prompt_ids_1\": instance_tokens_1,\n",
    "            \"instance_prompt_ids_2\": instance_tokens_2,\n",
    "            \"class_prompt_ids_1\": class_tokens_1,\n",
    "            \"class_prompt_ids_2\": class_tokens_2\n",
    "        }\n",
    "\n",
    "# Fun√ß√£o para aplicar LoRA no transformer\n",
    "def setup_lora_layers(transformer):\n",
    "    \"\"\"Aplicar LoRA nos m√≥dulos de aten√ß√£o do transformer FLUX (compat√≠vel FLUX.1-dev Colab)\"\"\"\n",
    "    lora_attn_procs = {}\n",
    "    for name, module in transformer.named_modules():\n",
    "        if \"attn\" in name and hasattr(module, \"to_k\"):\n",
    "            # Use only the processor key as required by set_attn_processor\n",
    "            lora_attn_procs[f\"{name}.processor\"] = LoRAAttnProcessor2_0()\n",
    "    transformer.set_attn_processor(lora_attn_procs)\n",
    "    return lora_attn_procs\n",
    "\n",
    "# Fun√ß√£o principal de treinamento\n",
    "def train_flux_lora():\n",
    "    \"\"\"Executar treinamento FLUX LoRA completo\"\"\"\n",
    "    try:\n",
    "        print(\"\\nüì• FASE 1: Carregando modelo base...\")\n",
    "        \n",
    "        # Carregar pipeline FLUX\n",
    "        pipe = FluxPipeline.from_pretrained(\n",
    "            BASE_MODEL_ID,\n",
    "            torch_dtype=dtype,\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\" if dtype == torch.float16 else None\n",
    "        )\n",
    "        \n",
    "        pipe = pipe.to(device)\n",
    "        print(f\"‚úÖ Pipeline FLUX carregado: {BASE_MODEL_ID}\")\n",
    "        \n",
    "        # Extrair componentes\n",
    "        transformer = pipe.transformer\n",
    "        vae = pipe.vae\n",
    "        text_encoder = pipe.text_encoder\n",
    "        text_encoder_2 = pipe.text_encoder_2\n",
    "        tokenizer_1 = pipe.tokenizer\n",
    "        tokenizer_2 = pipe.tokenizer_2\n",
    "        scheduler = pipe.scheduler\n",
    "        \n",
    "        print(\"‚úÖ Componentes extra√≠dos com sucesso\")\n",
    "        \n",
    "        print(\"\\nüì• FASE 2: Configurando LoRA...\")\n",
    "        \n",
    "        # Configurar LoRA\n",
    "        lora_attn_procs = setup_lora_layers(transformer)\n",
    "        # Debug: inspecionar atributos dos processadores\n",
    "        print('--- DEBUG: Inspecionando attn_processors ---')\n",
    "        for name, proc in getattr(transformer, 'attn_processors', {}).items():\n",
    "            print(f'Processador: {name} | Tipo: {type(proc)}')\n",
    "            print('Atributos:', dir(proc))\n",
    "        print('--- FIM DEBUG ---')\n",
    "        # Obter par√¢metros trein√°veis corretamente dos LoRAAttnProcessor2_0\n",
    "        trainable_params = []\n",
    "        for proc in getattr(transformer, 'attn_processors', {}).values():\n",
    "            for attr in ['to_q_lora', 'to_k_lora', 'to_v_lora', 'to_out_lora']:\n",
    "                lora_layer = getattr(proc, attr, None)\n",
    "                if lora_layer is not None:\n",
    "                    for p in lora_layer.parameters():\n",
    "                        if p.requires_grad:\n",
    "                            trainable_params.append(p)\n",
    "        print(f\"‚úÖ LoRA configurado: {len(trainable_params)} par√¢metros trein√°veis\")\n",
    "        print(f\"üîß Rank: {LORA_RANK}, Alpha: {LORA_ALPHA}, Dropout: {LORA_DROPOUT}\")\n",
    "        \n",
    "        print(\"\\nüì• FASE 3: Preparando dataset...\")\n",
    "        \n",
    "        # Criar dataset\n",
    "        dataset = FluxLoRADataset(\n",
    "            COLAB_DATASET_PATH,\n",
    "            INSTANCE_PROMPT,\n",
    "            CLASS_PROMPT,\n",
    "            tokenizer_1,\n",
    "            tokenizer_2,\n",
    "            size=RESOLUTION\n",
    "        )\n",
    "        \n",
    "        # Criar dataloader\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Dataset preparado: {len(dataset)} imagens\")\n",
    "        \n",
    "        print(\"\\nüì• FASE 4: Configurando otimizador...\")\n",
    "        \n",
    "        # Configurar otimizador\n",
    "        if USE_8BIT_ADAM:\n",
    "            try:\n",
    "                import bitsandbytes as bnb\n",
    "                optimizer_cls = bnb.optim.AdamW8bit\n",
    "                print(\"‚úÖ Usando AdamW8bit\")\n",
    "            except ImportError:\n",
    "                optimizer_cls = torch.optim.AdamW\n",
    "                print(\"‚ö†Ô∏è Fallback para AdamW padr√£o\")\n",
    "        else:\n",
    "            optimizer_cls = torch.optim.AdamW\n",
    "        \n",
    "        optimizer = optimizer_cls(\n",
    "            trainable_params,\n",
    "            lr=LEARNING_RATE,\n",
    "            betas=(ADAM_BETA1, ADAM_BETA2),\n",
    "            weight_decay=ADAM_WEIGHT_DECAY,\n",
    "            eps=ADAM_EPSILON\n",
    "        )\n",
    "        \n",
    "        # Configurar scheduler\n",
    "        lr_scheduler = get_scheduler(\n",
    "            LR_SCHEDULER,\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=LR_WARMUP_STEPS,\n",
    "            num_training_steps=MAX_TRAIN_STEPS,\n",
    "            num_cycles=LR_NUM_CYCLES\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Otimizador configurado: {optimizer_cls.__name__}\")\n",
    "        print(f\"üìà Scheduler: {LR_SCHEDULER}\")\n",
    "        \n",
    "        print(\"\\nüì• FASE 5: Iniciando treinamento...\")\n",
    "        \n",
    "        # Preparar modelos para treinamento\n",
    "        transformer.train()\n",
    "        vae.eval()\n",
    "        text_encoder.eval()\n",
    "        text_encoder_2.eval()\n",
    "        \n",
    "        # Desabilitar gradientes nos modelos n√£o trein√°veis\n",
    "        vae.requires_grad_(False)\n",
    "        text_encoder.requires_grad_(False)\n",
    "        text_encoder_2.requires_grad_(False)\n",
    "        \n",
    "        # Vari√°veis de controle\n",
    "        global_step = 0\n",
    "        training_stats = []\n",
    "        \n",
    "        # Progress bar\n",
    "        progress_bar = tqdm(total=MAX_TRAIN_STEPS, desc=\"Treinamento\")\n",
    "        \n",
    "        # Loop de treinamento\n",
    "        epoch = 0\n",
    "        while global_step < MAX_TRAIN_STEPS:\n",
    "            epoch += 1\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(dataloader):\n",
    "                if global_step >= MAX_TRAIN_STEPS:\n",
    "                    break\n",
    "                \n",
    "                # Mover batch para device\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, dtype=dtype)\n",
    "                instance_prompt_ids_1 = batch[\"instance_prompt_ids_1\"].to(device)\n",
    "                instance_prompt_ids_2 = batch[\"instance_prompt_ids_2\"].to(device)\n",
    "                class_prompt_ids_1 = batch[\"class_prompt_ids_1\"].to(device)\n",
    "                class_prompt_ids_2 = batch[\"class_prompt_ids_2\"].to(device)\n",
    "                \n",
    "                # Encode imagens com VAE\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "                    latents = latents * vae.config.scaling_factor\n",
    "                \n",
    "                # Adicionar ru√≠do\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(\n",
    "                    0, scheduler.config.num_train_timesteps,\n",
    "                    (latents.shape[0],), device=device\n",
    "                ).long()\n",
    "                \n",
    "                noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                # Encode prompts\n",
    "                with torch.no_grad():\n",
    "                    # Text encoder 1 (CLIP)\n",
    "                    encoder_hidden_states_1 = text_encoder(instance_prompt_ids_1)[0]\n",
    "                    class_hidden_states_1 = text_encoder(class_prompt_ids_1)[0]\n",
    "                    \n",
    "                    # Text encoder 2 (T5)\n",
    "                    encoder_hidden_states_2 = text_encoder_2(instance_prompt_ids_2)[0]\n",
    "                    class_hidden_states_2 = text_encoder_2(class_prompt_ids_2)[0]\n",
    "                \n",
    "                # Prediction do transformer\n",
    "                model_pred = transformer(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=encoder_hidden_states_1,\n",
    "                    pooled_projections=encoder_hidden_states_2.mean(dim=1),\n",
    "                    return_dict=False\n",
    "                )[0]\n",
    "                \n",
    "                # Calcular loss\n",
    "                if scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = scheduler.get_velocity(latents, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(f\"Prediction type {scheduler.config.prediction_type} n√£o suportado\")\n",
    "                \n",
    "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "                \n",
    "                # Prior preservation loss\n",
    "                if PRIOR_LOSS_WEIGHT > 0:\n",
    "                    with torch.no_grad():\n",
    "                        class_model_pred = transformer(\n",
    "                            noisy_latents,\n",
    "                            timesteps,\n",
    "                            encoder_hidden_states=class_hidden_states_1,\n",
    "                            pooled_projections=class_hidden_states_2.mean(dim=1),\n",
    "                            return_dict=False\n",
    "                        )[0]\n",
    "                    \n",
    "                    prior_loss = F.mse_loss(class_model_pred.float(), target.float(), reduction=\"mean\")\n",
    "                    loss = loss + PRIOR_LOSS_WEIGHT * prior_loss\n",
    "                \n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                \n",
    "                if (global_step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    # Gradient clipping\n",
    "                    if MAX_GRAD_NORM > 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(trainable_params, MAX_GRAD_NORM)\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                # Logging\n",
    "                current_loss = loss.detach().item()\n",
    "                epoch_loss += current_loss\n",
    "                \n",
    "                # Update progress\n",
    "                progress_bar.set_postfix({\n",
    "                    \"loss\": f\"{current_loss:.4f}\",\n",
    "                    \"lr\": f\"{lr_scheduler.get_last_lr()[0]:.2e}\",\n",
    "                    \"epoch\": epoch\n",
    "                })\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if global_step > 0 and global_step % SAVE_STEPS == 0:\n",
    "                    save_checkpoint(transformer, lora_layers, global_step)\n",
    "                \n",
    "                # Armazenar estat√≠sticas\n",
    "                if global_step % 10 == 0:\n",
    "                    training_stats.append({\n",
    "                        \"step\": global_step,\n",
    "                        \"loss\": current_loss,\n",
    "                        \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                        \"epoch\": epoch\n",
    "                    })\n",
    "                \n",
    "                global_step += 1\n",
    "                \n",
    "                # Limpeza de mem√≥ria\n",
    "                if global_step % 50 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        progress_bar.close()\n",
    "        \n",
    "        print(\"\\nüì• FASE 6: Salvando modelo final...\")\n",
    "        \n",
    "        # Salvar LoRA final\n",
    "        save_final_lora(lora_layers, training_stats)\n",
    "        \n",
    "        print(\"\\nüéâ TREINAMENTO CONCLU√çDO COM SUCESSO!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERRO DURANTE TREINAMENTO: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    finally:\n",
    "        # Limpeza\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "def save_checkpoint(transformer, lora_layers, step):\n",
    "    \"\"\"Salvar checkpoint do treinamento\"\"\"\n",
    "    checkpoint_dir = f\"{COLAB_OUTPUT_PATH}/checkpoint-{step}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Salvar LoRA weights\n",
    "    lora_state_dict = lora_layers.state_dict()\n",
    "    checkpoint_path = f\"{checkpoint_dir}/pytorch_lora_weights.safetensors\"\n",
    "    st.save_file(lora_state_dict, checkpoint_path)\n",
    "    \n",
    "    print(f\"‚úÖ Checkpoint salvo: {checkpoint_path}\")\n",
    "\n",
    "def save_final_lora(lora_layers, training_stats):\n",
    "    \"\"\"Salvar LoRA final e metadados\"\"\"\n",
    "    os.makedirs(COLAB_OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    # Salvar LoRA weights\n",
    "    lora_state_dict = lora_layers.state_dict()\n",
    "    lora_path = f\"{COLAB_OUTPUT_PATH}/valentina_identity_lora.safetensors\"\n",
    "    st.save_file(lora_state_dict, lora_path)\n",
    "    \n",
    "    # Salvar metadados\n",
    "    metadata = {\n",
    "        \"model_name\": \"Valentina Identity LoRA\",\n",
    "        \"base_model\": BASE_MODEL_ID,\n",
    "        \"training_config\": {\n",
    "            \"lora_rank\": LORA_RANK,\n",
    "            \"lora_alpha\": LORA_ALPHA,\n",
    "            \"lora_dropout\": LORA_DROPOUT,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"max_train_steps\": MAX_TRAIN_STEPS,\n",
    "            \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "            \"resolution\": RESOLUTION,\n",
    "            \"instance_prompt\": INSTANCE_PROMPT,\n",
    "            \"class_prompt\": CLASS_PROMPT\n",
    "        },\n",
    "        \"training_stats\": training_stats[-100:],  # √öltimas 100 entradas\n",
    "        \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    with open(f\"{COLAB_OUTPUT_PATH}/training_metadata.json\", 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ LoRA salvo: {lora_path}\")\n",
    "    print(f\"üìä Tamanho: {os.path.getsize(lora_path) / 1024 / 1024:.1f}MB\")\n",
    "\n",
    "# Executar treinamento\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"üéØ Configura√ß√µes de treinamento:\")\n",
    "    print(f\"   üìä Steps: {MAX_TRAIN_STEPS}\")\n",
    "    print(f\"   üß† LoRA Rank: {LORA_RANK}\")\n",
    "    print(f\"   üìà Learning Rate: {LEARNING_RATE}\")\n",
    "    print(f\"   üé≠ Trigger: '{INSTANCE_PROMPT}'\")\n",
    "    print(f\"   üìê Resolu√ß√£o: {RESOLUTION}x{RESOLUTION}\")\n",
    "    \n",
    "    success = train_flux_lora()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n‚úÖ TREINAMENTO FINALIZADO COM SUCESSO!\")\n",
    "        print(\"üéâ LoRA de identidade facial da Valentina est√° pronta!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå TREINAMENTO FALHOU!\")\n",
    "        print(\"üìã Verifique os logs acima para detalhes do erro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb51bbb",
   "metadata": {},
   "source": [
    "## C√©lula 8: Processamento dos Resultados de Identidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ C√âLULA 8: PROCESSAMENTO E DOWNLOAD DOS RESULTADOS\n",
    "# An√°lise, valida√ß√£o e empacotamento da LoRA de identidade facial\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from google.colab import files\n",
    "import torch\n",
    "import safetensors.torch as st\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "print(\"üì¶ PROCESSAMENTO DOS RESULTADOS DE IDENTIDADE FACIAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def analyze_training_results():\n",
    "    \"\"\"Analisar resultados do treinamento\"\"\"\n",
    "    print(\"üîç Analisando resultados do treinamento...\")\n",
    "    \n",
    "    results = {\n",
    "        \"lora_file\": None,\n",
    "        \"checkpoints\": [],\n",
    "        \"metadata\": None,\n",
    "        \"file_sizes\": {},\n",
    "        \"training_completed\": False\n",
    "    }\n",
    "    \n",
    "    # Verificar arquivo principal da LoRA\n",
    "    main_lora_path = f\"{COLAB_OUTPUT_PATH}/valentina_identity_lora.safetensors\"\n",
    "    if os.path.exists(main_lora_path):\n",
    "        results[\"lora_file\"] = main_lora_path\n",
    "        results[\"file_sizes\"][\"main_lora\"] = os.path.getsize(main_lora_path)\n",
    "        results[\"training_completed\"] = True\n",
    "        print(f\"‚úÖ LoRA principal encontrada: {os.path.basename(main_lora_path)}\")\n",
    "        print(f\"   üíæ Tamanho: {results['file_sizes']['main_lora'] / 1024 / 1024:.1f}MB\")\n",
    "    \n",
    "    # Verificar checkpoints\n",
    "    if os.path.exists(COLAB_OUTPUT_PATH):\n",
    "        for item in os.listdir(COLAB_OUTPUT_PATH):\n",
    "            if item.startswith(\"checkpoint-\"):\n",
    "                checkpoint_path = f\"{COLAB_OUTPUT_PATH}/{item}\"\n",
    "                lora_checkpoint = f\"{checkpoint_path}/pytorch_lora_weights.safetensors\"\n",
    "                if os.path.exists(lora_checkpoint):\n",
    "                    results[\"checkpoints\"].append({\n",
    "                        \"step\": int(item.split(\"-\")[1]),\n",
    "                        \"path\": lora_checkpoint,\n",
    "                        \"size\": os.path.getsize(lora_checkpoint)\n",
    "                    })\n",
    "    \n",
    "    # Ordenar checkpoints por step\n",
    "    results[\"checkpoints\"].sort(key=lambda x: x[\"step\"])\n",
    "    print(f\"üìã Checkpoints encontrados: {len(results['checkpoints'])}\")\n",
    "    \n",
    "    # Verificar metadados\n",
    "    metadata_path = f\"{COLAB_OUTPUT_PATH}/training_metadata.json\"\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            results[\"metadata\"] = json.load(f)\n",
    "        print(\"‚úÖ Metadados de treinamento encontrados\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def validate_lora_weights(lora_path):\n",
    "    \"\"\"Validar integridade dos pesos da LoRA\"\"\"\n",
    "    print(f\"üîç Validando LoRA: {os.path.basename(lora_path)}\")\n",
    "    \n",
    "    try:\n",
    "        # Carregar e analisar pesos\n",
    "        state_dict = st.load_file(lora_path)\n",
    "        \n",
    "        # Estat√≠sticas dos pesos\n",
    "        total_params = 0\n",
    "        layer_info = {}\n",
    "        \n",
    "        for key, tensor in state_dict.items():\n",
    "            total_params += tensor.numel()\n",
    "            layer_type = key.split(\".\")[-2] if \".\" in key else \"unknown\"\n",
    "            \n",
    "            if layer_type not in layer_info:\n",
    "                layer_info[layer_type] = {\"count\": 0, \"params\": 0}\n",
    "            \n",
    "            layer_info[layer_type][\"count\"] += 1\n",
    "            layer_info[layer_type][\"params\"] += tensor.numel()\n",
    "        \n",
    "        print(f\"   üìä Par√¢metros totais: {total_params:,}\")\n",
    "        print(f\"   üèóÔ∏è Camadas LoRA:\")\n",
    "        for layer_type, info in layer_info.items():\n",
    "            print(f\"      ‚Ä¢ {layer_type}: {info['count']} camadas, {info['params']:,} par√¢metros\")\n",
    "        \n",
    "        # Verificar se h√° pesos n√£o-zero\n",
    "        non_zero_weights = 0\n",
    "        total_weights = 0\n",
    "        \n",
    "        for tensor in state_dict.values():\n",
    "            non_zero_weights += (tensor != 0).sum().item()\n",
    "            total_weights += tensor.numel()\n",
    "        \n",
    "        non_zero_ratio = non_zero_weights / total_weights if total_weights > 0 else 0\n",
    "        print(f\"   üé® Pesos n√£o-zero: {non_zero_ratio:.1%}\")\n",
    "        \n",
    "        # Verificar range dos valores\n",
    "        all_values = torch.cat([tensor.flatten() for tensor in state_dict.values()])\n",
    "        print(f\"   üìä Range de valores: [{all_values.min():.4f}, {all_values.max():.4f}]\")\n",
    "        print(f\"   üìä M√©dia: {all_values.mean():.4f}, Std: {all_values.std():.4f}\")\n",
    "        \n",
    "        validation_result = {\n",
    "            \"total_params\": total_params,\n",
    "            \"layer_info\": layer_info,\n",
    "            \"non_zero_ratio\": non_zero_ratio,\n",
    "            \"value_range\": [float(all_values.min()), float(all_values.max())],\n",
    "            \"mean\": float(all_values.mean()),\n",
    "            \"std\": float(all_values.std()),\n",
    "            \"valid\": True\n",
    "        }\n",
    "        \n",
    "        print(\"‚úÖ LoRA validada com sucesso!\")\n",
    "        return validation_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro na valida√ß√£o: {e}\")\n",
    "        return {\"valid\": False, \"error\": str(e)}\n",
    "\n",
    "def create_test_generation(lora_path):\n",
    "    \"\"\"Criar gera√ß√£o de teste com a LoRA\"\"\"\n",
    "    print(\"üé® Criando gera√ß√£o de teste...\")\n",
    "    \n",
    "    try:\n",
    "        # Carregar pipeline\n",
    "        pipe = FluxPipeline.from_pretrained(\n",
    "            BASE_MODEL_ID,\n",
    "            torch_dtype=torch.float16,\n",
    "            use_safetensors=True\n",
    "        )\n",
    "        \n",
    "        # Carregar LoRA\n",
    "        pipe.load_lora_weights(lora_path)\n",
    "        pipe = pipe.to(\"cuda\")\n",
    "        \n",
    "        # Prompt de teste\n",
    "        test_prompt = f\"a professional portrait photo of {INSTANCE_PROMPT.replace('a photo of ', '')}, high quality, detailed face, studio lighting\"\n",
    "        \n",
    "        print(f\"   üìù Prompt: {test_prompt}\")\n",
    "        \n",
    "        # Gerar imagem\n",
    "        with torch.no_grad():\n",
    "            image = pipe(\n",
    "                prompt=test_prompt,\n",
    "                height=1024,\n",
    "                width=1024,\n",
    "                num_inference_steps=25,\n",
    "                guidance_scale=7.5,\n",
    "                generator=torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "            ).images[0]\n",
    "        \n",
    "        # Salvar imagem de teste\n",
    "        test_image_path = f\"{COLAB_OUTPUT_PATH}/test_generation.png\"\n",
    "        image.save(test_image_path)\n",
    "        \n",
    "        print(f\"‚úÖ Gera√ß√£o de teste salva: {test_image_path}\")\n",
    "        \n",
    "        # Limpar mem√≥ria\n",
    "        del pipe\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return test_image_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro na gera√ß√£o de teste: {e}\")\n",
    "        print(\"   üìù Isso pode ser normal se o Colab n√£o tiver VRAM suficiente\")\n",
    "        return None\n",
    "\n",
    "def create_documentation(results, validation_data):\n",
    "    \"\"\"Criar documenta√ß√£o completa da LoRA\"\"\"\n",
    "    print(\"üìù Criando documenta√ß√£o...\")\n",
    "    \n",
    "    # Calcular estat√≠sticas do dataset\n",
    "    instance_images_path = f\"{COLAB_DATASET_PATH}/instance_images\"\n",
    "    if not os.path.exists(instance_images_path):\n",
    "        instance_images_path = COLAB_DATASET_PATH\n",
    "    \n",
    "    image_files = []\n",
    "    for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\"]:\n",
    "        image_files.extend(list(Path(instance_images_path).glob(ext)))\n",
    "    \n",
    "    # Criar README detalhado\n",
    "    readme_content = f\"\"\"# Valentina Identity LoRA - FLUX.1-dev\n",
    "\n",
    "## üéØ Objetivo\n",
    "Esta LoRA foi treinada especificamente para capturar e preservar a **identidade facial da Valentina**, focando em:\n",
    "- Consist√™ncia de caracter√≠sticas faciais\n",
    "- Preserva√ß√£o da identidade visual\n",
    "- Gera√ß√£o de alta qualidade com FLUX.1-dev\n",
    "\n",
    "## üìã Informa√ß√µes T√©cnicas\n",
    "\n",
    "### Modelo Base\n",
    "- **Arquitetura**: FLUX.1-dev\n",
    "- **Desenvolvedor**: Black Forest Labs\n",
    "- **Tipo**: Modelo de difus√£o transformer\n",
    "\n",
    "### Configura√ß√µes de Treinamento\n",
    "- **LoRA Rank**: {LORA_RANK}\n",
    "- **LoRA Alpha**: {LORA_ALPHA}\n",
    "- **LoRA Dropout**: {LORA_DROPOUT}\n",
    "- **Learning Rate**: {LEARNING_RATE}\n",
    "- **Steps de Treinamento**: {MAX_TRAIN_STEPS}\n",
    "- **Batch Size**: {TRAIN_BATCH_SIZE}\n",
    "- **Resolu√ß√£o**: {RESOLUTION}x{RESOLUTION}\n",
    "- **Imagens de Treinamento**: {len(image_files)}\n",
    "\n",
    "### Dataset\n",
    "- **Fonte**: Gerado pelo valentina_dataset_generator_colab.ipynb\n",
    "- **Tipo**: Imagens com seeds sequenciais para m√°xima consist√™ncia\n",
    "- **Foco**: Identidade facial pura (n√£o NSFW)\n",
    "- **Caracter√≠sticas preservadas**:\n",
    "  - Idade: 25 anos\n",
    "  - Formato do rosto: Oval\n",
    "  - Olhos: Amendoados castanho-escuros\n",
    "  - L√°bios: Carnudos com arco do cupido\n",
    "  - Pele: Dourada mediterr√¢nea\n",
    "  - Cabelo: Castanho m√©dio com reflexos dourados\n",
    "  - Tatuagens: NENHUMA (pele limpa)\n",
    "\n",
    "## üöÄ Como Usar\n",
    "\n",
    "### mflux (macOS)\n",
    "```bash\n",
    "mflux-generate \\\\\n",
    "    --model \"/path/to/flux.1-dev\" \\\\\n",
    "    --lora \"valentina_identity_lora.safetensors\" \\\\\n",
    "    --lora-scale 0.8 \\\\\n",
    "    --prompt \"a photo of vltna woman, [seu prompt]\" \\\\\n",
    "    --steps 25 \\\\\n",
    "    --height 1024 \\\\\n",
    "    --width 1024\n",
    "```\n",
    "\n",
    "### ComfyUI\n",
    "1. Coloque o arquivo .safetensors na pasta `models/loras/`\n",
    "2. Use o n√≥ \"Load LoRA\" no workflow\n",
    "3. Configure o peso entre 0.7-1.0\n",
    "4. Use o trigger word `vltna woman` nos prompts\n",
    "\n",
    "### Automatic1111\n",
    "1. Coloque o arquivo na pasta `models/Lora/`\n",
    "2. Use `<lora:valentina_identity_lora:0.8>` no prompt\n",
    "3. Inclua `vltna woman` no prompt\n",
    "\n",
    "## üé® Prompts Recomendados\n",
    "\n",
    "### Para Identidade/Retratos\n",
    "```\n",
    "a photo of vltna woman, professional portrait, studio lighting, detailed face\n",
    "vltna woman, natural smile, elegant pose, high quality photography\n",
    "a photo of vltna woman, looking at camera, soft lighting, photorealistic\n",
    "```\n",
    "\n",
    "### Para Situa√ß√µes Espec√≠ficas\n",
    "```\n",
    "vltna woman, business attire, office environment, confident expression\n",
    "a photo of vltna woman, casual outfit, outdoor scene, natural lighting\n",
    "vltna woman, elegant dress, formal event, sophisticated pose\n",
    "```\n",
    "\n",
    "## ‚öôÔ∏è Configura√ß√µes Recomendadas\n",
    "\n",
    "| Par√¢metro | Valor Recomendado | Observa√ß√µes |\n",
    "|-----------|------------------|-------------|\n",
    "| LoRA Weight | 0.7 - 1.0 | Comece com 0.8 |\n",
    "| Steps | 25 - 35 | 25 √© ideal para velocidade |\n",
    "| CFG Scale | 7 - 9 | 7.5 √© um bom ponto de partida |\n",
    "| Resolu√ß√£o | 1024x1024 | Resolu√ß√£o de treinamento |\n",
    "| Sampler | DPM++ 2M | Para FLUX.1-dev |\n",
    "\n",
    "## üìã Resultados de Valida√ß√£o\n",
    "\n",
    "- **Par√¢metros Totais**: {validation_data.get('total_params', 'N/A'):,}\n",
    "- **Pesos N√£o-Zero**: {validation_data.get('non_zero_ratio', 0):.1%}\n",
    "- **Range de Valores**: [{validation_data.get('value_range', [0, 0])[0]:.4f}, {validation_data.get('value_range', [0, 0])[1]:.4f}]\n",
    "- **M√©dia dos Pesos**: {validation_data.get('mean', 0):.4f}\n",
    "\n",
    "## ‚ö†Ô∏è Notas Importantes\n",
    "\n",
    "1. **Foco em Identidade**: Esta LoRA foi treinada especificamente para preservar a identidade facial da Valentina\n",
    "2. **N√£o NSFW**: Otimizada para gera√ß√£o de conte√∫do n√£o expl√≠cito\n",
    "3. **Consist√™ncia**: Use seeds semelhantes para manter consist√™ncia entre gera√ß√µes\n",
    "4. **Qualidade**: Melhor qualidade com resolu√ß√µes altas (1024x1024 ou superior)\n",
    "5. **Trigger Word**: Sempre inclua `vltna woman` para ativar a identidade\n",
    "\n",
    "## üìä Performance\n",
    "\n",
    "- **VRAM Necess√°ria**: ~8-12GB para gera√ß√£o 1024x1024\n",
    "- **Tempo de Gera√ß√£o**: ~30-60 segundos (dependendo da GPU)\n",
    "- **Qualidade**: Alta fidelidade √† identidade da Valentina\n",
    "- **Estabilidade**: Testada em m√∫ltiplas gera√ß√µes\n",
    "\n",
    "## üîÑ Atualiza√ß√µes\n",
    "\n",
    "- **Vers√£o**: 1.0 (Treinamento inicial de identidade)\n",
    "- **Data**: {results.get('metadata', {}).get('timestamp', 'N/A')}\n",
    "- **Status**: Pronta para uso em produ√ß√£o\n",
    "\n",
    "## üìû Suporte\n",
    "\n",
    "Para problemas ou d√∫vidas:\n",
    "1. Verifique se est√° usando o modelo base correto (FLUX.1-dev)\n",
    "2. Confirme que o trigger word `vltna woman` est√° no prompt\n",
    "3. Ajuste o peso da LoRA entre 0.7-1.0\n",
    "4. Use as configura√ß√µes recomendadas acima\n",
    "\n",
    "---\n",
    "\n",
    "**Gerado pelo Sistema de Treinamento Valentina LoRA**  \n",
    "**Dataset**: valentina_identity_4lora_dataset_flux  \n",
    "**Notebook**: valentina_colab_facial_lora_trainer_flux.ipynb\n",
    "\"\"\"\n",
    "    \n",
    "    return readme_content\n",
    "\n",
    "def create_final_package(results, validation_data):\n",
    "    \"\"\"Criar pacote final para download\"\"\"\n",
    "    print(\"üì¶ Criando pacote final...\")\n",
    "    \n",
    "    # Criar diret√≥rio do pacote\n",
    "    package_dir = f\"{COLAB_OUTPUT_PATH}/valentina_identity_lora_package\"\n",
    "    os.makedirs(package_dir, exist_ok=True)\n",
    "    \n",
    "    # Copiar LoRA principal\n",
    "    if results[\"lora_file\"]:\n",
    "        final_lora_name = \"valentina_identity_lora.safetensors\"\n",
    "        shutil.copy2(results[\"lora_file\"], f\"{package_dir}/{final_lora_name}\")\n",
    "        print(f\"‚úÖ LoRA copiada: {final_lora_name}\")\n",
    "    \n",
    "    # Criar documenta√ß√£o\n",
    "    readme_content = create_documentation(results, validation_data)\n",
    "    with open(f\"{package_dir}/README.md\", 'w', encoding='utf-8') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    # Criar arquivo de configura√ß√£o JSON\n",
    "    config = {\n",
    "        \"model_info\": {\n",
    "            \"name\": \"Valentina Identity LoRA\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"base_model\": BASE_MODEL_ID,\n",
    "            \"type\": \"identity_lora\",\n",
    "            \"architecture\": \"flux_transformer\"\n",
    "        },\n",
    "        \"training_config\": {\n",
    "            \"lora_rank\": LORA_RANK,\n",
    "            \"lora_alpha\": LORA_ALPHA,\n",
    "            \"lora_dropout\": LORA_DROPOUT,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"max_steps\": MAX_TRAIN_STEPS,\n",
    "            \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "            \"resolution\": RESOLUTION,\n",
    "            \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS\n",
    "        },\n",
    "        \"character_profile\": {\n",
    "            \"name\": \"Valentina Moreau\",\n",
    "            \"age\": 25,\n",
    "            \"face_shape\": \"oval\",\n",
    "            \"eyes\": \"amendoados castanho-escuros\",\n",
    "            \"lips\": \"carnudos com arco do cupido\",\n",
    "            \"skin\": \"dourada mediterr√¢nea\",\n",
    "            \"hair\": \"castanho m√©dio com reflexos dourados\",\n",
    "            \"body_type\": \"curvil√≠neo\",\n",
    "            \"tattoos\": \"nenhuma - pele limpa\"\n",
    "        },\n",
    "        \"usage\": {\n",
    "            \"trigger_word\": INSTANCE_PROMPT,\n",
    "            \"recommended_weight\": \"0.7-1.0\",\n",
    "            \"compatible_software\": [\"mflux\", \"ComfyUI\", \"Automatic1111\"],\n",
    "            \"optimal_resolution\": \"1024x1024\",\n",
    "            \"recommended_steps\": \"25-35\"\n",
    "        },\n",
    "        \"validation\": validation_data,\n",
    "        \"created_at\": results.get(\"metadata\", {}).get(\"timestamp\", \"unknown\")\n",
    "    }\n",
    "    \n",
    "    with open(f\"{package_dir}/config.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Copiar imagem de teste se existir\n",
    "    test_image_path = f\"{COLAB_OUTPUT_PATH}/test_generation.png\"\n",
    "    if os.path.exists(test_image_path):\n",
    "        shutil.copy2(test_image_path, f\"{package_dir}/sample_generation.png\")\n",
    "        print(\"‚úÖ Imagem de teste inclu√≠da\")\n",
    "    \n",
    "    # Copiar metadados de treinamento se existir\n",
    "    metadata_path = f\"{COLAB_OUTPUT_PATH}/training_metadata.json\"\n",
    "    if os.path.exists(metadata_path):\n",
    "        shutil.copy2(metadata_path, f\"{package_dir}/training_metadata.json\")\n",
    "    \n",
    "    # Criar arquivo ZIP\n",
    "    zip_filename = f\"{COLAB_OUTPUT_PATH}/valentina_identity_lora_final.zip\"\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:\n",
    "        for root, dirs, files in os.walk(package_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arc_name = os.path.relpath(file_path, package_dir)\n",
    "                zipf.write(file_path, arc_name)\n",
    "    \n",
    "    package_size = os.path.getsize(zip_filename) / 1024 / 1024\n",
    "    print(f\"‚úÖ Pacote criado: {zip_filename}\")\n",
    "    print(f\"üíæ Tamanho do pacote: {package_size:.1f}MB\")\n",
    "    \n",
    "    return zip_filename, package_size\n",
    "\n",
    "def display_summary(results, validation_data, package_info):\n",
    "    \"\"\"Exibir resumo final\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéâ RESUMO FINAL DO TREINAMENTO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if results[\"training_completed\"]:\n",
    "        print(\"‚úÖ STATUS: TREINAMENTO CONCLU√çDO COM SUCESSO\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è STATUS: TREINAMENTO INCOMPLETO OU COM PROBLEMAS\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüéØ MODELO:\")\n",
    "    print(f\"   ‚Ä¢ Nome: Valentina Identity LoRA\")\n",
    "    print(f\"   ‚Ä¢ Base: {BASE_MODEL_ID}\")\n",
    "    print(f\"   ‚Ä¢ Tipo: Identidade Facial (n√£o NSFW)\")\n",
    "    print(f\"   ‚Ä¢ Trigger: '{INSTANCE_PROMPT}'\")\n",
    "    \n",
    "    print(f\"\\nüìä ESTAT√çSTICAS:\")\n",
    "    if validation_data.get('valid'):\n",
    "        print(f\"   ‚Ä¢ Par√¢metros LoRA: {validation_data['total_params']:,}\")\n",
    "        print(f\"   ‚Ä¢ Pesos ativos: {validation_data['non_zero_ratio']:.1%}\")\n",
    "        print(f\"   ‚Ä¢ Tamanho do arquivo: {results['file_sizes']['main_lora'] / 1024 / 1024:.1f}MB\")\n",
    "    \n",
    "    print(f\"\\nüîß CONFIGURA√á√ïES:\")\n",
    "    print(f\"   ‚Ä¢ LoRA Rank: {LORA_RANK}\")\n",
    "    print(f\"   ‚Ä¢ LoRA Alpha: {LORA_ALPHA}\")\n",
    "    print(f\"   ‚Ä¢ Steps: {MAX_TRAIN_STEPS}\")\n",
    "    print(f\"   ‚Ä¢ Learning Rate: {LEARNING_RATE}\")\n",
    "    print(f\"   ‚Ä¢ Resolu√ß√£o: {RESOLUTION}x{RESOLUTION}\")\n",
    "    \n",
    "    if package_info:\n",
    "        zip_file, size = package_info\n",
    "        print(f\"\\nüì¶ PACOTE FINAL:\")\n",
    "        print(f\"   ‚Ä¢ Arquivo: {os.path.basename(zip_file)}\")\n",
    "        print(f\"   ‚Ä¢ Tamanho: {size:.1f}MB\")\n",
    "        print(f\"   ‚Ä¢ Conte√∫do: LoRA + Documenta√ß√£o + Config\")\n",
    "    \n",
    "    print(f\"\\nüöÄ COMO USAR:\")\n",
    "    print(f\"   1. Baixe o arquivo ZIP\")\n",
    "    print(f\"   2. Extraia e use o arquivo .safetensors\")\n",
    "    print(f\"   3. Configure peso 0.7-1.0 no seu software\")\n",
    "    print(f\"   4. Use '{INSTANCE_PROMPT}' nos prompts\")\n",
    "    print(f\"   5. Leia o README.md para detalhes\")\n",
    "    \n",
    "    print(f\"\\nüé® CARACTER√çSTICAS PRESERVADAS:\")\n",
    "    print(f\"   ‚Ä¢ Idade: 25 anos\")\n",
    "    print(f\"   ‚Ä¢ Rosto: Oval\")\n",
    "    print(f\"   ‚Ä¢ Olhos: Amendoados castanho-escuros\")\n",
    "    print(f\"   ‚Ä¢ Pele: Dourada mediterr√¢nea, sem tatuagens\")\n",
    "    print(f\"   ‚Ä¢ Cabelo: Castanho m√©dio com reflexos\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Execu√ß√£o principal\n",
    "if __name__ == \"__main__\":\n",
    "    # Analisar resultados\n",
    "    results = analyze_training_results()\n",
    "    \n",
    "    if not results[\"training_completed\"]:\n",
    "        print(\"‚ùå ERRO: Treinamento n√£o foi conclu√≠do ou LoRA n√£o foi encontrada!\")\n",
    "        print(\"\\nüîç Verificando diret√≥rio de sa√≠da:\")\n",
    "        if os.path.exists(COLAB_OUTPUT_PATH):\n",
    "            for item in os.listdir(COLAB_OUTPUT_PATH):\n",
    "                item_path = os.path.join(COLAB_OUTPUT_PATH, item)\n",
    "                if os.path.isfile(item_path):\n",
    "                    size = os.path.getsize(item_path) / 1024 / 1024\n",
    "                    print(f\"   üìÑ {item} ({size:.1f}MB)\")\n",
    "                else:\n",
    "                    print(f\"   üìÅ {item}/\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Diret√≥rio n√£o existe: {COLAB_OUTPUT_PATH}\")\n",
    "    else:\n",
    "        # Validar LoRA\n",
    "        validation_data = validate_lora_weights(results[\"lora_file\"])\n",
    "        \n",
    "        # Criar gera√ß√£o de teste (opcional)\n",
    "        test_image = create_test_generation(results[\"lora_file\"])\n",
    "        \n",
    "        # Criar pacote final\n",
    "        package_info = create_final_package(results, validation_data)\n",
    "        \n",
    "        # Exibir resumo\n",
    "        display_summary(results, validation_data, package_info)\n",
    "        \n",
    "        # Download do pacote\n",
    "        if package_info:\n",
    "            zip_file, _ = package_info\n",
    "            print(f\"\\nüì• INICIANDO DOWNLOAD...\")\n",
    "            files.download(zip_file)\n",
    "            print(f\"‚úÖ Download conclu√≠do: {os.path.basename(zip_file)}\")\n",
    "            \n",
    "            print(f\"\\nüéâ PARAB√âNS!\")\n",
    "            print(f\"Sua LoRA de identidade facial da Valentina est√° pronta para uso!\")\n",
    "            print(f\"\\nPr√≥ximos passos:\")\n",
    "            print(f\"1. Extraia o arquivo ZIP baixado\")\n",
    "            print(f\"2. Leia o README.md para instru√ß√µes detalhadas\")\n",
    "            print(f\"3. Teste com o prompt: 'a photo of vltna woman, portrait'\")\n",
    "            print(f\"4. Ajuste o peso da LoRA conforme necess√°rio (0.7-1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28326a4e",
   "metadata": {},
   "source": [
    "## C√©lula 9: Limpeza (Opcional)\n",
    "Descomente para remover arquivos grandes e economizar espa√ßo no Colab se for continuar usando o runtime ap√≥s o treinamento de identidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f23b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Limpando arquivos de treinamento de identidade...\")\n",
    "# !rm -rf {COLAB_MODELS_PATH}/{BASE_MODEL_ID.split('/')[-1]} # Remove o cache do modelo base\n",
    "# !rm -f {DATASET_ZIP_FILENAME} # Remove o dataset ZIP\n",
    "# !rm -rf {COLAB_DATASET_PATH}/* # Limpa imagens extra√≠das do dataset de identidade\n",
    "# print(\"Limpeza conclu√≠da (arquivos de modelo e dataset de identidade).\")\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
