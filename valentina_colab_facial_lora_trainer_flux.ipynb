{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9023aa15",
   "metadata": {},
   "source": [
    "# üéØ Valentina Facial LoRA Trainer - Google Colab Edition\n",
    "\n",
    "Este notebook treina uma LoRA de alta qualidade para a identidade facial da Valentina usando FLUX.1-dev + NSFW checkpoint + Midjourney LoRA no Google Colab.\n",
    "\n",
    "## Stack de Modelos:\n",
    "- **Base**: FLUX.1-dev (via checkpoint NSFW)\n",
    "- **Checkpoint**: `John6666/nsfw-master-flux-lora-merged-with-flux1-dev-fp16-v10-fp8-flux`\n",
    "- **Style LoRA**: Midjourney LoRA (aplicada como base)\n",
    "- **Output**: LoRA facial da Valentina para uso local com mflux\n",
    "\n",
    "‚ö†Ô∏è **Importante**: Execute as c√©lulas em ordem e aguarde a conclus√£o de cada etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configura√ß√µes Principais ---\n",
    "BASE_MODEL_ID = \"John6666/nsfw-master-flux-lora-merged-with-flux1-dev-fp16-v10-fp8-flux\" # Checkpoint NSFW (FLUX.1-dev mergeado)\n",
    "MIDJOURNEY_LORA_FILENAME = \"midjourney_LORA.safetensors\" # Nome do arquivo da LoRA Midjourney\n",
    "DATASET_ZIP_FILENAME = \"valentina_dataset.zip\" # Nome do arquivo zip do dataset\n",
    "\n",
    "# --- Caminhos no Ambiente Colab ---\n",
    "COLAB_MODELS_PATH = \"/content/models\"\n",
    "COLAB_DATASET_PATH = \"/content/dataset\"\n",
    "COLAB_OUTPUT_PATH = \"/content/output_lora\"\n",
    "COLAB_LOGS_PATH = \"/content/logs\"\n",
    "\n",
    "# --- Par√¢metros de Treinamento da LoRA Facial ---\n",
    "INSTANCE_PROMPT = \"a photo of vltna woman\" # Token √∫nico para Valentina\n",
    "CLASS_PROMPT = \"a photo of a woman\" # Prompt de classe para regulariza√ß√£o\n",
    "\n",
    "# Configura√ß√µes de resolu√ß√£o e qualidade\n",
    "RESOLUTION = 1024\n",
    "CENTER_CROP = True\n",
    "RANDOM_FLIP = False # Evitar flip para manter consist√™ncia facial\n",
    "\n",
    "# Batch sizes otimizados para T4/V100\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 8 # Batch efetivo de 8\n",
    "\n",
    "# Learning rates otimizados para LoRA facial\n",
    "LEARNING_RATE = 8e-5 # Menor para preservar caracter√≠sticas faciais\n",
    "UNET_LR = 8e-5\n",
    "TEXT_ENCODER_LR = 5e-6 # Muito menor para text encoder\n",
    "\n",
    "# Scheduler e warmup\n",
    "LR_SCHEDULER = \"cosine_with_restarts\"\n",
    "LR_WARMUP_STEPS = 100\n",
    "LR_NUM_CYCLES = 1\n",
    "\n",
    "# Steps de treinamento (calculado: ~100-150 steps por imagem)\n",
    "MAX_TRAIN_STEPS = 1000 # Para 7 imagens = ~142 steps/imagem\n",
    "SAVE_STEPS = 200\n",
    "VALIDATION_EPOCHS = 5\n",
    "\n",
    "# Arquitetura LoRA otimizada\n",
    "LORA_RANK = 128 # Rank alto para melhor qualidade facial\n",
    "LORA_ALPHA = 64 # Alpha = rank/2 para estabilidade\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "# Configura√ß√µes de precis√£o e otimiza√ß√£o\n",
    "MIXED_PRECISION = \"bf16\" # Melhor qualidade se suportado\n",
    "USE_8BIT_ADAM = True\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_WEIGHT_DECAY = 0.01\n",
    "ADAM_EPSILON = 1e-8\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Memory optimization\n",
    "GRADIENT_CHECKPOINTING = True\n",
    "ENABLE_XFORMERS = True\n",
    "USE_CPU_OFFLOAD = False # Manter na GPU para velocidade\n",
    "\n",
    "# Regulariza√ß√£o e qualidade\n",
    "PRIOR_LOSS_WEIGHT = 1.0\n",
    "SNR_GAMMA = 5.0 # Para melhor qualidade com ru√≠do\n",
    "\n",
    "# Checkpointing\n",
    "CHECKPOINTING_STEPS = 200\n",
    "CHECKPOINTS_TOTAL_LIMIT = 3\n",
    "RESUME_FROM_CHECKPOINT = None\n",
    "\n",
    "# Seeds para reprodutibilidade\n",
    "SEED = 42\n",
    "\n",
    "# Configura√ß√µes da Midjourney LoRA\n",
    "USE_MIDJOURNEY_LORA = True\n",
    "MIDJOURNEY_LORA_WEIGHT = 0.8 # Peso maior para estilo consistente\n",
    "\n",
    "print(\"‚öôÔ∏è Configura√ß√µes otimizadas carregadas:\")\n",
    "print(f\"üìä Steps totais: {MAX_TRAIN_STEPS}\")\n",
    "print(f\"üéØ Batch efetivo: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"üß† LoRA Rank: {LORA_RANK}\")\n",
    "print(f\"üé® Usar Midjourney LoRA: {USE_MIDJOURNEY_LORA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80027a40",
   "metadata": {},
   "source": [
    "## C√©lula 2: Setup do Ambiente Colab (Depend√™ncias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09fe240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Instala√ß√£o Otimizada de Depend√™ncias\n",
    "print(\"üì¶ Instalando depend√™ncias otimizadas para treinamento FLUX...\")\n",
    "\n",
    "# Instalar PyTorch com CUDA\n",
    "!pip install -q --upgrade pip setuptools wheel\n",
    "!pip install -q torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Instalar diffusers e depend√™ncias principais\n",
    "!pip install -q git+https://github.com/huggingface/diffusers.git\n",
    "!pip install -q transformers==4.36.0\n",
    "!pip install -q accelerate==0.25.0\n",
    "!pip install -q peft==0.7.1\n",
    "!pip install -q safetensors==0.4.1\n",
    "\n",
    "# Depend√™ncias de otimiza√ß√£o\n",
    "!pip install -q xformers==0.0.22.post7 --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q bitsandbytes==0.41.3\n",
    "!pip install -q triton==2.1.0\n",
    "\n",
    "# Utilit√°rios\n",
    "!pip install -q opencv-python-headless==4.8.1.78\n",
    "!pip install -q pillow==10.1.0\n",
    "!pip install -q imageio==2.31.6\n",
    "!pip install -q ftfy==6.1.1\n",
    "\n",
    "# Logging e monitoramento\n",
    "!pip install -q tensorboard==2.15.1\n",
    "!pip install -q wandb==0.16.1\n",
    "\n",
    "# Hugging Face\n",
    "!pip install -q huggingface_hub==0.19.4\n",
    "!pip install -q datasets==2.14.7\n",
    "\n",
    "print(\"‚úÖ Todas as depend√™ncias instaladas com sucesso!\")\n",
    "print(\"üî• Ambiente otimizado para treinamento de LoRA FLUX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d8568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üñ•Ô∏è Verifica√ß√£o e Otimiza√ß√£o da GPU\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"üîç Verificando configura√ß√£o da GPU...\")\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits\n",
    "\n",
    "# Detectar capacidades da GPU\n",
    "gpu_name = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=name\", \"--format=csv,noheader\"]).decode().strip()\n",
    "print(f\"üì± GPU detectada: {gpu_name}\")\n",
    "\n",
    "# Ajustar configura√ß√µes baseado na GPU\n",
    "if \"T4\" in gpu_name:\n",
    "    print(\"üîß Otimiza√ß√µes para Tesla T4\")\n",
    "    MIXED_PRECISION = \"fp16\"  # T4 funciona melhor com fp16\n",
    "    TRAIN_BATCH_SIZE = 1\n",
    "    GRADIENT_ACCUMULATION_STEPS = 6\n",
    "elif \"V100\" in gpu_name:\n",
    "    print(\"üîß Otimiza√ß√µes para V100\")\n",
    "    MIXED_PRECISION = \"fp16\"\n",
    "    TRAIN_BATCH_SIZE = 1\n",
    "    GRADIENT_ACCUMULATION_STEPS = 8\n",
    "elif \"A100\" in gpu_name or \"H100\" in gpu_name:\n",
    "    print(\"üîß Otimiza√ß√µes para GPU high-end\")\n",
    "    MIXED_PRECISION = \"bf16\"  # Melhor qualidade\n",
    "    TRAIN_BATCH_SIZE = 2\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "else:\n",
    "    print(\"üîß Configura√ß√µes padr√£o\")\n",
    "    MIXED_PRECISION = \"fp16\"\n",
    "\n",
    "print(f\"‚úÖ Configura√ß√µes ajustadas: {MIXED_PRECISION}, batch={TRAIN_BATCH_SIZE}\")\n",
    "\n",
    "# Limpar cache da GPU\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"üßπ Cache da GPU limpo. Mem√≥ria livre: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "\n",
    "print(\"Instalando depend√™ncias... Por favor, aguarde.\")\n",
    "!pip install -q diffusers transformers accelerate bitsandbytes safetensors peft xformers huggingface_hub torch torchvision torchaudio --upgrade\n",
    "\n",
    "print(\"Depend√™ncias instaladas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "print(\"Por favor, fa√ßa login na sua conta Hugging Face para baixar os modelos.\")\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b111f",
   "metadata": {},
   "source": [
    "## C√©lula 4: Cria√ß√£o da Estrutura de Diret√≥rios no Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c01b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Cria√ß√£o da Estrutura de Diret√≥rios\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Criar todos os diret√≥rios necess√°rios\n",
    "directories = [\n",
    "    COLAB_MODELS_PATH,\n",
    "    COLAB_DATASET_PATH,\n",
    "    COLAB_OUTPUT_PATH,\n",
    "    COLAB_LOGS_PATH,\n",
    "    f\"{COLAB_DATASET_PATH}/instance_images\",\n",
    "    f\"{COLAB_DATASET_PATH}/class_images\",\n",
    "    f\"{COLAB_OUTPUT_PATH}/checkpoints\",\n",
    "    f\"{COLAB_OUTPUT_PATH}/samples\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"üìÇ Criado: {directory}\")\n",
    "\n",
    "print(\"\\n‚úÖ Estrutura de diret√≥rios criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69f9df",
   "metadata": {},
   "source": [
    "## C√©lula 5: Upload e Prepara√ß√£o dos Dados (Dataset e LoRA Midjourney)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d00b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì§ Upload e Prepara√ß√£o dos Dados\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "print(\"üì• === UPLOAD DO DATASET ===\")\n",
    "print(f\"Por favor, fa√ßa upload do arquivo: {DATASET_ZIP_FILENAME}\")\n",
    "uploaded_dataset = files.upload()\n",
    "\n",
    "if DATASET_ZIP_FILENAME in uploaded_dataset:\n",
    "    print(f\"üì¶ Extraindo {DATASET_ZIP_FILENAME}...\")\n",
    "    with zipfile.ZipFile(DATASET_ZIP_FILENAME, 'r') as zip_ref:\n",
    "        zip_ref.extractall(COLAB_DATASET_PATH)\n",
    "    \n",
    "    # Encontrar as imagens extra√≠das\n",
    "    instance_images_path = f\"{COLAB_DATASET_PATH}/instance_images\"\n",
    "    \n",
    "    # Verificar estrutura do dataset\n",
    "    extracted_files = []\n",
    "    for root, dirs, files in os.walk(COLAB_DATASET_PATH):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                extracted_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"üñºÔ∏è Encontradas {len(extracted_files)} imagens\")\n",
    "    \n",
    "    # Mover imagens para pasta de inst√¢ncia\n",
    "    for i, img_path in enumerate(extracted_files):\n",
    "        new_path = f\"{instance_images_path}/valentina_{i:03d}.png\"\n",
    "        \n",
    "        # Converter e redimensionar se necess√°rio\n",
    "        with Image.open(img_path) as img:\n",
    "            # Converter para RGB se necess√°rio\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Redimensionar mantendo aspect ratio\n",
    "            img.thumbnail((RESOLUTION, RESOLUTION), Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Criar imagem quadrada com padding\n",
    "            new_img = Image.new('RGB', (RESOLUTION, RESOLUTION), (255, 255, 255))\n",
    "            paste_x = (RESOLUTION - img.width) // 2\n",
    "            paste_y = (RESOLUTION - img.height) // 2\n",
    "            new_img.paste(img, (paste_x, paste_y))\n",
    "            \n",
    "            new_img.save(new_path, 'PNG', quality=95)\n",
    "        \n",
    "        print(f\"‚úÖ Processada: {os.path.basename(img_path)} -> {os.path.basename(new_path)}\")\n",
    "    \n",
    "    print(f\"\\nüìä Dataset preparado: {len(extracted_files)} imagens de inst√¢ncia\")\n",
    "    !ls -la {instance_images_path}\n",
    "else:\n",
    "    print(f\"‚ùå ERRO: {DATASET_ZIP_FILENAME} n√£o encontrado!\")\n",
    "    raise FileNotFoundError(\"Dataset n√£o foi enviado\")\n",
    "\n",
    "print(\"\\nüì• === UPLOAD DA MIDJOURNEY LORA ===\")\n",
    "if USE_MIDJOURNEY_LORA:\n",
    "    print(f\"Por favor, fa√ßa upload do arquivo: {MIDJOURNEY_LORA_FILENAME}\")\n",
    "    uploaded_lora = files.upload()\n",
    "    \n",
    "    if MIDJOURNEY_LORA_FILENAME in uploaded_lora:\n",
    "        midjourney_path = f\"{COLAB_MODELS_PATH}/{MIDJOURNEY_LORA_FILENAME}\"\n",
    "        shutil.move(MIDJOURNEY_LORA_FILENAME, midjourney_path)\n",
    "        print(f\"‚úÖ Midjourney LoRA salva em: {midjourney_path}\")\n",
    "        \n",
    "        # Verificar tamanho do arquivo\n",
    "        file_size = os.path.getsize(midjourney_path) / (1024*1024)\n",
    "        print(f\"üìè Tamanho: {file_size:.1f}MB\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Midjourney LoRA n√£o enviada. Treinamento continuar√° sem ela.\")\n",
    "        USE_MIDJOURNEY_LORA = False\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Midjourney LoRA desabilitada nas configura√ß√µes\")\n",
    "\n",
    "print(\"\\n‚úÖ Upload e prepara√ß√£o dos dados conclu√≠dos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249e809",
   "metadata": {},
   "source": [
    "## C√©lula 6: Prepara√ß√£o do Modelo Base para Treinamento da LoRA Facial\n",
    "Carrega o checkpoint NSFW, aplica e funde a LoRA Midjourney (se habilitada e compat√≠vel), e salva este novo modelo como base para o treinamento da LoRA facial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "import gc\n",
    "\n",
    "print(\"üöÄ Iniciando prepara√ß√£o do modelo base...\")\n",
    "\n",
    "# Configurar dtype baseado na GPU\n",
    "if torch.cuda.is_available():\n",
    "    if torch.cuda.get_device_capability()[0] >= 8:  # Ampere+\n",
    "        model_dtype = torch.bfloat16\n",
    "        print(\"üíé Usando bfloat16 (GPU Ampere+)\")\n",
    "    else:\n",
    "        model_dtype = torch.float16\n",
    "        print(\"‚ö° Usando float16 (GPU older)\")\n",
    "else:\n",
    "    model_dtype = torch.float32\n",
    "    print(\"üêå Usando float32 (CPU)\")\n",
    "\n",
    "try:\n",
    "    print(f\"üì¶ Carregando modelo base: {BASE_MODEL_ID}\")\n",
    "    \n",
    "    # Carregar pipeline com configura√ß√µes otimizadas\n",
    "    pipeline = FluxPipeline.from_pretrained(\n",
    "        BASE_MODEL_ID,\n",
    "        torch_dtype=model_dtype,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\" if model_dtype != torch.float32 else None\n",
    "    )\n",
    "    \n",
    "    print(\"üîß Configurando pipeline para treinamento...\")\n",
    "    pipeline.to(\"cuda\")\n",
    "    pipeline.enable_model_cpu_offload()  # Otimiza√ß√£o de mem√≥ria\n",
    "    \n",
    "    if USE_MIDJOURNEY_LORA and os.path.exists(f\"{COLAB_MODELS_PATH}/{MIDJOURNEY_LORA_FILENAME}\"):\n",
    "        print(\"üé® Aplicando Midjourney LoRA...\")\n",
    "        \n",
    "        try:\n",
    "            # Carregar LoRA Midjourney\n",
    "            pipeline.load_lora_weights(\n",
    "                f\"{COLAB_MODELS_PATH}/{MIDJOURNEY_LORA_FILENAME}\",\n",
    "                adapter_name=\"midjourney\"\n",
    "            )\n",
    "            \n",
    "            # Configurar peso da LoRA\n",
    "            pipeline.set_adapters([\"midjourney\"], adapter_weights=[MIDJOURNEY_LORA_WEIGHT])\n",
    "            \n",
    "            print(f\"‚úÖ Midjourney LoRA aplicada com peso {MIDJOURNEY_LORA_WEIGHT}\")\n",
    "            \n",
    "            # Salvar modelo intermedi√°rio\n",
    "            intermediate_path = f\"{COLAB_MODELS_PATH}/base_with_midjourney\"\n",
    "            print(f\"üíæ Salvando modelo intermedi√°rio em: {intermediate_path}\")\n",
    "            \n",
    "            pipeline.save_pretrained(\n",
    "                intermediate_path,\n",
    "                safe_serialization=True,\n",
    "                variant=\"fp16\" if model_dtype != torch.float32 else None\n",
    "            )\n",
    "            \n",
    "            model_path = intermediate_path\n",
    "            print(\"üéØ Modelo intermedi√°rio salvo com sucesso!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Erro ao aplicar Midjourney LoRA: {e}\")\n",
    "            print(\"üìù Continuando com modelo base original...\")\n",
    "            model_path = BASE_MODEL_ID\n",
    "            USE_MIDJOURNEY_LORA = False\n",
    "    else:\n",
    "        print(\"üìã Usando modelo base original (sem Midjourney LoRA)\")\n",
    "        model_path = BASE_MODEL_ID\n",
    "    \n",
    "    print(f\"üèÅ Modelo final preparado: {model_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro cr√≠tico na prepara√ß√£o do modelo: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    # Limpeza de mem√≥ria\n",
    "    if 'pipeline' in locals():\n",
    "        del pipeline\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üßπ Mem√≥ria limpa\")\n",
    "\n",
    "print(\"\\n‚úÖ Prepara√ß√£o do modelo conclu√≠da!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd0a718",
   "metadata": {},
   "source": [
    "## C√©lula 7: Treinamento da LoRA Facial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baixando script de treinamento Dreambooth LoRA para FLUX...\")\n",
    "!wget https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_flux.py -O {COLAB_TRAINING_SCRIPT_PATH}\n",
    "\n",
    "print(\"Construindo comando de treinamento...\")\n",
    "training_command = f\"\n",
    "\n",
    "accelerate launch {COLAB_TRAINING_SCRIPT_PATH} \\\n",
    "  --pretrained_model_name_or_path='{model_to_train_on}' \\\n",
    "  --instance_data_dir='{COLAB_ACTUAL_DATASET_IMAGES_PATH}' \\\n",
    "  --output_dir='{COLAB_OUTPUT_PATH}' \\\n",
    "  --instance_prompt='{INSTANCE_PROMPT}' \\\n",
    "  --resolution={RESOLUTION} \\\n",
    "  --train_batch_size={TRAIN_BATCH_SIZE} \\\n",
    "  --gradient_accumulation_steps={GRADIENT_ACCUMULATION_STEPS} \\\n",
    "  --learning_rate={LEARNING_RATE} \\\n",
    "  --lr_scheduler='{LR_SCHEDULER}' \\\n",
    "  --lr_warmup_steps={LR_WARMUP_STEPS} \\\n",
    "  --max_train_steps={MAX_TRAIN_STEPS} \\\n",
    "  --lora_rank={LORA_RANK_FACIAL} \\\n",
    "  --seed={SEED} \\\n",
    "  --mixed_precision='{MIXED_PRECISION}' \\\n",
    "  --checkpointing_steps={CHECKPOINTING_STEPS} \\\n",
    "  --checkpoints_total_limit={CHECKPOINTS_TOTAL_LIMIT} \\\n",
    "  --validation_prompt='A photo of {INSTANCE_PROMPT} in a vibrant city at night' \\\n",
    "  --validation_epochs=10 \\\n",
    "  --report_to='tensorboard'\n",
    "\n",
    ",\n",
    ",\n",
    "if GRADIENT_CHECKPOINTING:\n",
    "    training_command += \" \\\n",
    "  --gradient_checkpointing\"\n",
    "if ENABLE_XFORMERS:\n",
    "    training_command += \" \\\n",
    "  --enable_xformers_memory_efficient_attention\"\n",
    "\n",
    "print(\"Comando de Treinamento:\")\n",
    "print(training_command)\n",
    "\n",
    "print(\"\n",
    "Iniciando treinamento... Isso pode levar um tempo consider√°vel.\")\n",
    "# Executa o comando no shell\n",
    "!{training_command}\n",
    "\n",
    "# üöÇ Treinamento de LoRA de Alta Qualidade\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"üî• Iniciando treinamento da LoRA facial...\")\n",
    "\n",
    "# Baixar script de treinamento mais recente\n",
    "training_script_url = \"https://raw.githubusercontent.com/huggingface/diffusers/main/examples/dreambooth/train_dreambooth_lora_flux.py\"\n",
    "training_script_path = f\"{COLAB_LOGS_PATH}/train_dreambooth_lora_flux.py\"\n",
    "\n",
    "print(\"üì• Baixando script de treinamento otimizado...\")\n",
    "!wget -q {training_script_url} -O {training_script_path}\n",
    "\n",
    "# Verificar se o dataset est√° correto\n",
    "instance_images_path = f\"{COLAB_DATASET_PATH}/instance_images\"\n",
    "num_images = len([f for f in os.listdir(instance_images_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "print(f\"üìä Treinando com {num_images} imagens de inst√¢ncia\")\n",
    "\n",
    "# Calcular steps otimizados\n",
    "steps_per_image = 150  # Otimizado para qualidade facial\n",
    "calculated_steps = num_images * steps_per_image\n",
    "MAX_TRAIN_STEPS = min(calculated_steps, 1200)  # M√°ximo de 1200 steps\n",
    "\n",
    "print(f\"üéØ Steps calculados: {MAX_TRAIN_STEPS} ({steps_per_image} por imagem)\")\n",
    "\n",
    "# Construir comando de treinamento otimizado\n",
    "training_command = f\"\"\"\n",
    "accelerate launch {training_script_path} \\\n",
    "  --pretrained_model_name_or_path=\"{model_path}\" \\\n",
    "  --instance_data_dir=\"{instance_images_path}\" \\\n",
    "  --output_dir=\"{COLAB_OUTPUT_PATH}\" \\\n",
    "  --instance_prompt=\"{INSTANCE_PROMPT}\" \\\n",
    "  --resolution={RESOLUTION} \\\n",
    "  --train_batch_size={TRAIN_BATCH_SIZE} \\\n",
    "  --gradient_accumulation_steps={GRADIENT_ACCUMULATION_STEPS} \\\n",
    "  --learning_rate={LEARNING_RATE} \\\n",
    "  --lr_scheduler=\"{LR_SCHEDULER}\" \\\n",
    "  --lr_warmup_steps={LR_WARMUP_STEPS} \\\n",
    "  --lr_num_cycles={LR_NUM_CYCLES} \\\n",
    "  --max_train_steps={MAX_TRAIN_STEPS} \\\n",
    "  --checkpointing_steps={CHECKPOINTING_STEPS} \\\n",
    "  --checkpoints_total_limit={CHECKPOINTS_TOTAL_LIMIT} \\\n",
    "  --seed={SEED} \\\n",
    "  --mixed_precision=\"{MIXED_PRECISION}\" \\\n",
    "  --prior_loss_weight={PRIOR_LOSS_WEIGHT} \\\n",
    "  --snr_gamma={SNR_GAMMA} \\\n",
    "  --rank={LORA_RANK} \\\n",
    "  --alpha={LORA_ALPHA} \\\n",
    "  --target_modules=\"to_k\" \"to_q\" \"to_v\" \"to_out.0\" \\\n",
    "  --validation_prompt=\"{INSTANCE_PROMPT}, professional portrait, high quality, detailed\" \\\n",
    "  --validation_prompt=\"{INSTANCE_PROMPT}, smiling, natural lighting\" \\\n",
    "  --validation_prompt=\"{INSTANCE_PROMPT}, elegant pose, studio lighting\" \\\n",
    "  --num_validation_images=2 \\\n",
    "  --validation_epochs={VALIDATION_EPOCHS} \\\n",
    "  --logging_dir=\"{COLAB_LOGS_PATH}\" \\\n",
    "  --report_to=\"tensorboard\" \\\n",
    "  --push_to_hub=False\"\"\"\n",
    "\n",
    "# Adicionar flags condicionais\n",
    "if GRADIENT_CHECKPOINTING:\n",
    "    training_command += \" \\\\\n",
    "  --gradient_checkpointing\"\n",
    "\n",
    "if USE_8BIT_ADAM:\n",
    "    training_command += \" \\\\\n",
    "  --use_8bit_adam\"\n",
    "\n",
    "if ENABLE_XFORMERS:\n",
    "    training_command += \" \\\\\n",
    "  --enable_xformers_memory_efficient_attention\"\n",
    "\n",
    "if not RANDOM_FLIP:\n",
    "    training_command += \" \\\\\n",
    "  --no_hflip\"\n",
    "\n",
    "if CENTER_CROP:\n",
    "    training_command += \" \\\\\n",
    "  --center_crop\"\n",
    "\n",
    "print(\"üîß Comando de treinamento configurado:\")\n",
    "print(\"=\" * 50)\n",
    "print(training_command)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nüöÄ INICIANDO TREINAMENTO... (isso pode levar 30-60 minutos)\")\n",
    "print(\"üí° Acompanhe o progresso nas sa√≠das abaixo\")\n",
    "\n",
    "# Executar treinamento\n",
    "try:\n",
    "    result = subprocess.run(training_command, shell=True, capture_output=False, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"\\nüéâ TREINAMENTO CONCLU√çDO COM SUCESSO!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Treinamento finalizado com c√≥digo: {result.returncode}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Erro durante o treinamento: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\nüìä Verificando resultados...\")\n",
    "!ls -la {COLAB_OUTPUT_PATH}/\n",
    "print(\"\\nüìà Logs do Tensorboard dispon√≠veis em:\", COLAB_LOGS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb51bbb",
   "metadata": {},
   "source": [
    "## C√©lula 8: Resultados (Compactar e Baixar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c889a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Processamento e Download dos Resultados\n",
    "import shutil\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from google.colab import files\n",
    "import json\n",
    "\n",
    "print(\"üîç Analisando resultados do treinamento...\")\n",
    "\n",
    "# Verificar estrutura de sa√≠da\n",
    "print(f\"üìÅ Conte√∫do da pasta de sa√≠da:\")\n",
    "!ls -la {COLAB_OUTPUT_PATH}/\n",
    "\n",
    "# Encontrar a LoRA treinada\n",
    "lora_files = []\n",
    "for root, dirs, files in os.walk(COLAB_OUTPUT_PATH):\n",
    "    for file in files:\n",
    "        if file.endswith('.safetensors') and 'lora' in file.lower():\n",
    "            lora_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"\\nüéØ Arquivos LoRA encontrados: {len(lora_files)}\")\n",
    "for lora_file in lora_files:\n",
    "    size_mb = os.path.getsize(lora_file) / (1024*1024)\n",
    "    print(f\"  üìÑ {os.path.basename(lora_file)} ({size_mb:.1f}MB)\")\n",
    "\n",
    "# Identificar a LoRA final\n",
    "final_lora_path = None\n",
    "if lora_files:\n",
    "    # Procurar pela LoRA final (geralmente a maior ou a mais recente)\n",
    "    final_lora_path = max(lora_files, key=os.path.getmtime)\n",
    "    print(f\"\\n‚úÖ LoRA final identificada: {os.path.basename(final_lora_path)}\")\n",
    "else:\n",
    "    # Procurar em checkpoints\n",
    "    checkpoint_dirs = [d for d in os.listdir(COLAB_OUTPUT_PATH) if d.startswith('checkpoint-')]\n",
    "    if checkpoint_dirs:\n",
    "        latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split('-')[1]))\n",
    "        checkpoint_path = f\"{COLAB_OUTPUT_PATH}/{latest_checkpoint}\"\n",
    "        \n",
    "        # Procurar LoRA no checkpoint\n",
    "        checkpoint_lora = f\"{checkpoint_path}/pytorch_lora_weights.safetensors\"\n",
    "        if os.path.exists(checkpoint_lora):\n",
    "            final_lora_path = checkpoint_lora\n",
    "            print(f\"‚úÖ LoRA encontrada no checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "if final_lora_path:\n",
    "    # Preparar pacote final\n",
    "    final_package_dir = f\"{COLAB_OUTPUT_PATH}/valentina_facial_lora_package\"\n",
    "    os.makedirs(final_package_dir, exist_ok=True)\n",
    "    \n",
    "    # Copiar LoRA final\n",
    "    final_lora_name = \"valentina_facial_lora.safetensors\"\n",
    "    shutil.copy2(final_lora_path, f\"{final_package_dir}/{final_lora_name}\")\n",
    "    \n",
    "    # Criar arquivo de configura√ß√£o\n",
    "    config = {\n",
    "        \"model_name\": \"Valentina Facial LoRA\",\n",
    "        \"base_model\": BASE_MODEL_ID,\n",
    "        \"midjourney_lora_used\": USE_MIDJOURNEY_LORA,\n",
    "        \"midjourney_weight\": MIDJOURNEY_LORA_WEIGHT if USE_MIDJOURNEY_LORA else None,\n",
    "        \"training_params\": {\n",
    "            \"resolution\": RESOLUTION,\n",
    "            \"max_train_steps\": MAX_TRAIN_STEPS,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"lora_rank\": LORA_RANK,\n",
    "            \"lora_alpha\": LORA_ALPHA,\n",
    "            \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "            \"gradient_accumulation\": GRADIENT_ACCUMULATION_STEPS\n",
    "        },\n",
    "        \"usage_instructions\": {\n",
    "            \"trigger_word\": \"vltna woman\",\n",
    "            \"recommended_weight\": \"0.7-1.0\",\n",
    "            \"compatible_with\": \"mflux, ComfyUI, A1111\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(f\"{final_package_dir}/config.json\", 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    # Criar README\n",
    "    readme_content = f\"\"\"# Valentina Facial LoRA\n",
    "\n",
    "## Informa√ß√µes do Modelo\n",
    "- **Modelo Base**: {BASE_MODEL_ID}\n",
    "- **Tipo**: LoRA Facial para identidade da Valentina\n",
    "- **Resolu√ß√£o de Treinamento**: {RESOLUTION}x{RESOLUTION}\n",
    "- **Steps de Treinamento**: {MAX_TRAIN_STEPS}\n",
    "- **Trigger Word**: `vltna woman`\n",
    "\n",
    "## Como Usar\n",
    "\n",
    "### No mflux (MacBook):\n",
    "```bash\n",
    "mflux-generate \\\n",
    "    --model \"/path/to/base/model\" \\\n",
    "    --lora \"valentina_facial_lora.safetensors\" \\\n",
    "    --lora-scale 0.8 \\\n",
    "    --prompt \"a photo of vltna woman, [seu prompt aqui]\"\n",
    "```\n",
    "\n",
    "### Prompts Recomendados:\n",
    "- `a photo of vltna woman, professional portrait`\n",
    "- `vltna woman, elegant dress, studio lighting`\n",
    "- `a photo of vltna woman, natural smile, outdoor`\n",
    "\n",
    "### Configura√ß√µes Recomendadas:\n",
    "- **Peso da LoRA**: 0.7 - 1.0\n",
    "- **Steps**: 20-30\n",
    "- **CFG Scale**: 7-9\n",
    "\n",
    "## Notas\n",
    "- Treinada com {num_images} imagens de refer√™ncia\n",
    "- Compat√≠vel com FLUX.1-dev\n",
    "- Otimizada para gera√ß√£o facial de alta qualidade\n",
    "\"\"\"\n",
    "    \n",
    "    with open(f\"{final_package_dir}/README.md\", 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    # Copiar algumas imagens de valida√ß√£o se existirem\n",
    "    validation_dir = f\"{COLAB_OUTPUT_PATH}/validation_images\"\n",
    "    if os.path.exists(validation_dir):\n",
    "        package_validation_dir = f\"{final_package_dir}/sample_outputs\"\n",
    "        shutil.copytree(validation_dir, package_validation_dir, dirs_exist_ok=True)\n",
    "        print(\"üì∏ Imagens de valida√ß√£o inclu√≠das no pacote\")\n",
    "    \n",
    "    # Criar arquivo ZIP\n",
    "    zip_filename = \"valentina_facial_lora_package.zip\"\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        for root, dirs, files in os.walk(final_package_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arc_name = os.path.relpath(file_path, final_package_dir)\n",
    "                zipf.write(file_path, arc_name)\n",
    "    \n",
    "    print(f\"\\nüì¶ Pacote criado: {zip_filename}\")\n",
    "    print(f\"üìä Tamanho do pacote: {os.path.getsize(zip_filename) / (1024*1024):.1f}MB\")\n",
    "    \n",
    "    # Informa√ß√µes finais\n",
    "    print(\"\\nüéØ RESUMO DO TREINAMENTO:\")\n",
    "    print(f\"‚úÖ LoRA treinada com sucesso: {final_lora_name}\")\n",
    "    print(f\"üìà Steps completados: {MAX_TRAIN_STEPS}\")\n",
    "    print(f\"üé® Midjourney LoRA usada: {'Sim' if USE_MIDJOURNEY_LORA else 'N√£o'}\")\n",
    "    print(f\"üîß Rank da LoRA: {LORA_RANK}\")\n",
    "    print(f\"üí™ Trigger word: {INSTANCE_PROMPT}\")\n",
    "    \n",
    "    print(\"\\nüì• Baixando pacote completo...\")\n",
    "    files.download(zip_filename)\n",
    "    \n",
    "    print(\"\\nüéâ SUCESSO! LoRA da Valentina est√° pronta para uso no seu MacBook!\")\n",
    "    print(\"\\nüí° Instru√ß√µes:\")\n",
    "    print(\"1. Extraia o arquivo ZIP baixado\")\n",
    "    print(\"2. Coloque o arquivo .safetensors na pasta de LoRAs do mflux\")\n",
    "    print(\"3. Use o trigger word 'vltna woman' nos seus prompts\")\n",
    "    print(\"4. Comece com peso 0.8 e ajuste conforme necess√°rio\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå ERRO: Nenhuma LoRA foi encontrada nos resultados!\")\n",
    "    print(\"üîç Verificando logs para diagn√≥stico...\")\n",
    "    \n",
    "    # Tentar encontrar logs de erro\n",
    "    if os.path.exists(COLAB_LOGS_PATH):\n",
    "        !find {COLAB_LOGS_PATH} -name \"*.log\" -o -name \"events.out.tfevents.*\" | head -5\n",
    "    \n",
    "    print(\"\\nüìã Conte√∫do completo da pasta de sa√≠da:\")\n",
    "    !find {COLAB_OUTPUT_PATH} -type f | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28326a4e",
   "metadata": {},
   "source": [
    "## C√©lula 9: Limpeza (Opcional)\n",
    "Descomente para remover arquivos grandes e economizar espa√ßo no Colab se for continuar usando o runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f23b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Limpando arquivos baixados e modelos intermedi√°rios...\")\n",
    "# !rm -rf {COLAB_INTERMEDIATE_MODEL_SAVE_PATH} # Remove o modelo base com Midjourney fundida\n",
    "# !rm -rf {COLAB_MODELS_PATH}/{BASE_MODEL_ID.split('/')[-1]} # Remove o cache do modelo base original se baixado explicitamente\n",
    "# !rm -f {DATASET_ZIP_FILENAME}\n",
    "# !rm -f {COLAB_MIDJOURNEY_LORA_FULL_PATH} # Se n√£o for mais necess√°rio\n",
    "# !rm -rf {COLAB_DATASET_PATH}/* # Limpa imagens extra√≠das\n",
    "# print(\"Limpeza conclu√≠da (arquivos de modelo e dataset).\")\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
