{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9023aa15",
   "metadata": {},
   "source": [
    "# 🎯 Valentina Facial LoRA Trainer - Google Colab Edition\n",
    "\n",
    "Este notebook treina uma **LoRA de identidade facial** para a Valentina usando FLUX.1-dev + dataset otimizado no Google Colab.\n",
    "\n",
    "## 🎯 Foco: Identidade Visual (NÃO NSFW)\n",
    "- **Objetivo**: Gerar a Valentina com máxima consistência facial\n",
    "- **Dataset**: Imagens da pasta `valentina_identity_4lora_dataset_flux`\n",
    "- **Características**: 25 anos, rosto oval, olhos amendoados, sem tatuagens\n",
    "- **Trigger Word**: `vltna woman`\n",
    "\n",
    "## 📋 Stack Otimizada para Identidade:\n",
    "- **Base**: FLUX.1-dev (máxima qualidade facial)\n",
    "- **Dataset**: 18 imagens com seeds sequenciais (máxima consistência)\n",
    "- **Parâmetros**: Conservadores para preservar características faciais\n",
    "- **Output**: LoRA facial da Valentina para uso local com mflux\n",
    "\n",
    "⚠️ **Importante**: Execute as células em ordem e aguarde a conclusão de cada etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configurações Principais ---\n",
    "BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\" # Modelo base oficial para identidade\n",
    "DATASET_ZIP_FILENAME = \"valentina_identity_4lora_dataset_flux.zip\" # Dataset de identidade gerado\n",
    "\n",
    "# --- Caminhos no Ambiente Colab ---\n",
    "COLAB_MODELS_PATH = \"/content/models\"\n",
    "COLAB_DATASET_PATH = \"/content/dataset\"\n",
    "COLAB_OUTPUT_PATH = \"/content/output_lora\"\n",
    "COLAB_LOGS_PATH = \"/content/logs\"\n",
    "\n",
    "# --- Parâmetros de Treinamento da LoRA de Identidade ---\n",
    "INSTANCE_PROMPT = \"a photo of vltna woman\" # Token único para Valentina\n",
    "CLASS_PROMPT = \"a photo of a woman\" # Prompt de classe para regularização\n",
    "\n",
    "# Configurações de resolução e qualidade (baseado no dataset gerado)\n",
    "RESOLUTION = 1024  # Mesma resolução do dataset\n",
    "CENTER_CROP = True\n",
    "RANDOM_FLIP = False # NUNCA flipar para preservar características faciais\n",
    "\n",
    "# Batch sizes otimizados para identidade (conservadores)\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 4 # Batch efetivo menor para preservar detalhes\n",
    "\n",
    "# Learning rates CONSERVADORES para preservar identidade facial\n",
    "LEARNING_RATE = 5e-5 # Reduzido para preservar características faciais\n",
    "UNET_LR = 5e-5\n",
    "TEXT_ENCODER_LR = 3e-6 # Muito menor para text encoder\n",
    "\n",
    "# Scheduler e warmup otimizados para identidade\n",
    "LR_SCHEDULER = \"cosine_with_restarts\"\n",
    "LR_WARMUP_STEPS = 50 # Reduzido para dataset menor\n",
    "LR_NUM_CYCLES = 1\n",
    "\n",
    "# Steps de treinamento (calculado para dataset de identidade: ~80-120 steps por imagem)\n",
    "MAX_TRAIN_STEPS = 1500 # Para 18 imagens = ~83 steps/imagem (conservador)\n",
    "SAVE_STEPS = 300\n",
    "VALIDATION_EPOCHS = 3 # Reduzido para dataset menor\n",
    "\n",
    "# Arquitetura LoRA otimizada para IDENTIDADE FACIAL\n",
    "LORA_RANK = 64 # Rank menor para preservar identidade (era 128)\n",
    "LORA_ALPHA = 32 # Alpha = rank/2 para estabilidade\n",
    "LORA_DROPOUT = 0.05 # Dropout menor para melhor aprendizado\n",
    "\n",
    "# Configurações de precisão e otimização\n",
    "MIXED_PRECISION = \"bf16\" # Melhor qualidade se suportado\n",
    "USE_8BIT_ADAM = True\n",
    "ADAM_BETA1 = 0.9\n",
    "ADAM_BETA2 = 0.999\n",
    "ADAM_WEIGHT_DECAY = 0.01\n",
    "ADAM_EPSILON = 1e-8\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "# Memory optimization\n",
    "GRADIENT_CHECKPOINTING = True\n",
    "ENABLE_XFORMERS = True\n",
    "USE_CPU_OFFLOAD = False # Manter na GPU para velocidade\n",
    "\n",
    "# Regularização para IDENTIDADE FACIAL\n",
    "PRIOR_LOSS_WEIGHT = 1.0\n",
    "SNR_GAMMA = 5.0 # Para melhor qualidade com ruído\n",
    "\n",
    "# Checkpointing conservador\n",
    "CHECKPOINTING_STEPS = 300\n",
    "CHECKPOINTS_TOTAL_LIMIT = 3\n",
    "RESUME_FROM_CHECKPOINT = None\n",
    "\n",
    "# Seeds para reprodutibilidade (alinhado com dataset)\n",
    "SEED = 42 # Mesmo seed base do dataset\n",
    "\n",
    "# Configurações específicas para IDENTIDADE (não NSFW)\n",
    "USE_MIDJOURNEY_LORA = False # Desabilitado - foco em identidade pura\n",
    "VALIDATION_PROMPT = \"a photo of vltna woman, professional portrait photography\" # Prompt de validação para identidade\n",
    "\n",
    "print(\"⚙️ Configurações otimizadas para IDENTIDADE FACIAL carregadas:\")\n",
    "print(f\"📊 Steps totais: {MAX_TRAIN_STEPS}\")\n",
    "print(f\"🎯 Batch efetivo: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"🧠 LoRA Rank: {LORA_RANK} (reduzido para preservar identidade)\")\n",
    "print(f\"🎨 Base Model: {BASE_MODEL_ID}\")\n",
    "print(f\"💼 Dataset: {DATASET_ZIP_FILENAME}\")\n",
    "print(f\"🎭 Trigger: '{INSTANCE_PROMPT}'\")\n",
    "print(f\"🔬 Foco: IDENTIDADE FACIAL (não NSFW)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80027a40",
   "metadata": {},
   "source": [
    "## Célula 2: Setup do Ambiente Colab (Dependências)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09fe240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 INSTALAÇÃO OTIMIZADA DE DEPENDÊNCIAS VIA UV\n",
    "print(\"🔧 Instalando dependências usando uv para maior robustez...\")\n",
    "\n",
    "# 1. Instalar uv\n",
    "print(\"⚙️ Instalando uv...\")\n",
    "!pip install -q uv\n",
    "print(\"✅ uv instalado.\")\n",
    "\n",
    "# 2. Definir o conteúdo do requirements.txt baseado no pyproject.toml do mflux\n",
    "requirements_content = \"\"\"\\\n",
    "torch>=2.1.0,<2.4.0\n",
    "torchvision>=0.16.0,<0.19.0\n",
    "torchaudio>=2.1.0,<2.4.0\n",
    "diffusers[torch]>=0.27.0,<1.0\n",
    "transformers>=4.44.0,<5.0\n",
    "accelerate>=0.32.0,<1.0\n",
    "safetensors>=0.4.0,<0.5.0\n",
    "xformers>=0.0.27,<0.0.29\n",
    "pillow>=10.0.0,<11.0.0\n",
    "opencv-python>=4.10.0,<5.0\n",
    "huggingface-hub>=0.24.5,<1.0\n",
    "sentencepiece>=0.2.0,<0.3.0\n",
    "tokenizers>=0.19.0,<0.20.0\n",
    "protobuf>=5.27.0,<6.0.0\n",
    "numpy>=2.0.0,<3.0.0\n",
    "requests>=2.32.0,<3.0.0\n",
    "scipy>=1.14.0,<2.0.0\n",
    "matplotlib>=3.9.0,<4.0.0\n",
    "omegaconf>=2.3.0,<3.0.0\n",
    "einops>=0.8.0,<0.9.0\n",
    "invisible-watermark>=0.2.0,<0.3.0\n",
    "compel>=2.0.0,<3.0.0\n",
    "wandb>=0.17.0,<0.18.0\n",
    "peft>=0.12.0,<0.13.0\n",
    "bitsandbytes>=0.43.0,<0.44.0\n",
    "gradio>=4.39.0,<5.0.0\n",
    "albumentations>=1.4.0,<2.0.0\n",
    "imageio>=2.34.0,<3.0.0\n",
    "scikit-image>=0.24.0,<0.25.0\n",
    "tqdm>=4.66.0,<5.0.0\n",
    "ftfy>=6.2.0,<7.0.0\n",
    "tensorboard>=2.16.0,<3.0.0\n",
    "easydict>=1.13.0,<2.0.0\n",
    "clean-fid==0.1.35\n",
    "torchmetrics>=1.4.0,<2.0.0\n",
    "kornia>=0.7.0,<0.8.0\n",
    "lpips>=0.1.4,<0.2.0\n",
    "controlnet_aux>=0.0.7,<0.0.8\n",
    "segment-anything>=1.0.0,<2.0.0\n",
    "rembg[gpu]>=2.0.56,<3.0.0\n",
    "moviepy>=1.0.3,<2.0.0\n",
    "typer>=0.12.0,<0.13.0\n",
    "rich>=13.7.0,<14.0.0\n",
    "shellingham>=1.5.0,<2.0.0\n",
    "\"\"\"\n",
    "\n",
    "# 3. Criar o arquivo requirements.txt no ambiente do Colab\n",
    "requirements_file_path = \"/content/valentina_flux_requirements.txt\"\n",
    "with open(requirements_file_path, \"w\") as f:\n",
    "    f.write(requirements_content)\n",
    "print(f\"📄 {requirements_file_path} criado com dependências do mflux.\")\n",
    "\n",
    "# 4. Instalar dependências usando uv pip install\n",
    "print(f\"🚀 Instalando dependências de {requirements_file_path} com uv...\")\n",
    "print(\"🕒 Isso pode levar alguns minutos...\")\n",
    "!uv pip install -q -r {requirements_file_path} --extra-index-url https://download.pytorch.org/whl/cu121 --index-strategy unsafe-best-match\n",
    "\n",
    "print(\"✅ Dependências instaladas com sucesso usando uv!\")\n",
    "print(\"🎯 Ambiente otimizado para treinamento de LoRA FLUX\")\n",
    "\n",
    "# Verificação final de CUDA e PyTorch\n",
    "import torch\n",
    "print(f\"🔥 PyTorch: {torch.__version__}\")\n",
    "print(f\"🎮 CUDA disponível: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"🎯 GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"💾 VRAM total: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "    print(f\"🧹 VRAM livre: {torch.cuda.memory_allocated() / 1024**3:.1f}GB\")\n",
    "else:\n",
    "    print(\"⚠️ CUDA não detectado - usando CPU (MUITO LENTO)\")\n",
    "\n",
    "# Limpeza de memória inicial\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print(\"✅ Verificação de ambiente concluída!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d8568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🖥️ Verificação e Otimização da GPU\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "print(\"🔍 Verificando configuração da GPU...\")\n",
    "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits\n",
    "\n",
    "# Detectar capacidades da GPU\n",
    "gpu_name = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=name\", \"--format=csv,noheader\"]).decode().strip()\n",
    "print(f\"📱 GPU detectada: {gpu_name}\")\n",
    "\n",
    "# Ajustar configurações baseado na GPU\n",
    "if \"T4\" in gpu_name:\n",
    "    print(\"🔧 Otimizações para Tesla T4\")\n",
    "    MIXED_PRECISION = \"fp16\"  # T4 funciona melhor com fp16\n",
    "    TRAIN_BATCH_SIZE = 1\n",
    "    GRADIENT_ACCUMULATION_STEPS = 6\n",
    "elif \"V100\" in gpu_name:\n",
    "    print(\"🔧 Otimizações para V100\")\n",
    "    MIXED_PRECISION = \"fp16\"\n",
    "    TRAIN_BATCH_SIZE = 1\n",
    "    GRADIENT_ACCUMULATION_STEPS = 8\n",
    "elif \"A100\" in gpu_name or \"H100\" in gpu_name:\n",
    "    print(\"🔧 Otimizações para GPU high-end\")\n",
    "    MIXED_PRECISION = \"bf16\"  # Melhor qualidade\n",
    "    TRAIN_BATCH_SIZE = 2\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "else:\n",
    "    print(\"🔧 Configurações padrão\")\n",
    "    MIXED_PRECISION = \"fp16\"\n",
    "\n",
    "print(f\"✅ Configurações ajustadas: {MIXED_PRECISION}, batch={TRAIN_BATCH_SIZE}\")\n",
    "\n",
    "# Limpar cache da GPU\n",
    "torch.cuda.empty_cache()\n",
    "print(f\"🧹 Cache da GPU limpo. Memória livre: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "\n",
    "print(\"Instalando dependências... Por favor, aguarde.\")\n",
    "!pip install -q diffusers transformers accelerate bitsandbytes safetensors peft xformers huggingface_hub torch torchvision torchaudio --upgrade\n",
    "\n",
    "print(\"Dependências instaladas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06ebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "print(\"Por favor, faça login na sua conta Hugging Face para baixar os modelos.\")\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0b111f",
   "metadata": {},
   "source": [
    "## Célula 4: Criação da Estrutura de Diretórios no Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c01b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 Criação da Estrutura de Diretórios\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Criar todos os diretórios necessários\n",
    "directories = [\n",
    "    COLAB_MODELS_PATH,\n",
    "    COLAB_DATASET_PATH,\n",
    "    COLAB_OUTPUT_PATH,\n",
    "    COLAB_LOGS_PATH,\n",
    "    f\"{COLAB_DATASET_PATH}/instance_images\",\n",
    "    f\"{COLAB_DATASET_PATH}/class_images\",\n",
    "    f\"{COLAB_OUTPUT_PATH}/checkpoints\",\n",
    "    f\"{COLAB_OUTPUT_PATH}/samples\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"📂 Criado: {directory}\")\n",
    "\n",
    "print(\"\\n✅ Estrutura de diretórios criada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e69f9df",
   "metadata": {},
   "source": [
    "## Célula 5: Upload e Preparação do Dataset de Identidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d00b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📤 Upload e Preparação do Dataset de Identidade\n",
    "from google.colab import files\n",
    "import zipfile\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "print(\"📥 === UPLOAD DO DATASET DE IDENTIDADE ===\")\n",
    "print(f\"Por favor, faça upload do arquivo: {DATASET_ZIP_FILENAME}\")\n",
    "print(\"🧬 Dataset gerado pelo valentina_dataset_generator_colab.ipynb\")\n",
    "print(\"📊 Contém 18 imagens com seeds sequenciais para máxima consistência facial\")\n",
    "uploaded_dataset = files.upload()\n",
    "\n",
    "if DATASET_ZIP_FILENAME in uploaded_dataset:\n",
    "    print(f\"📦 Extraindo {DATASET_ZIP_FILENAME}...\")\n",
    "    with zipfile.ZipFile(DATASET_ZIP_FILENAME, 'r') as zip_ref:\n",
    "        zip_ref.extractall(COLAB_DATASET_PATH)\n",
    "    \n",
    "    # Encontrar as imagens extraídas\n",
    "    instance_images_path = f\"{COLAB_DATASET_PATH}/instance_images\"\n",
    "    \n",
    "    # Verificar estrutura do dataset\n",
    "    extracted_files = []\n",
    "    for root, dirs, files in os.walk(COLAB_DATASET_PATH):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                extracted_files.append(os.path.join(root, file))\n",
    "    \n",
    "    print(f\"🖼️ Encontradas {len(extracted_files)} imagens de identidade\")\n",
    "    \n",
    "    # Mover imagens para pasta de instância se necessário\n",
    "    if not os.path.exists(instance_images_path):\n",
    "        os.makedirs(instance_images_path, exist_ok=True)\n",
    "    \n",
    "    processed_count = 0\n",
    "    for i, img_path in enumerate(extracted_files):\n",
    "        new_path = f\"{instance_images_path}/valentina_{i:03d}.png\"\n",
    "        \n",
    "        # Verificar se já não está na pasta correta\n",
    "        if os.path.dirname(img_path) == instance_images_path:\n",
    "            continue\n",
    "            \n",
    "        # Converter e redimensionar se necessário\n",
    "        with Image.open(img_path) as img:\n",
    "            # Converter para RGB se necessário\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Verificar se já está no tamanho correto\n",
    "            if img.size == (RESOLUTION, RESOLUTION):\n",
    "                # Salvar direto se já está no tamanho certo\n",
    "                img.save(new_path, 'PNG', quality=95)\n",
    "            else:\n",
    "                # Redimensionar mantendo aspect ratio\n",
    "                img.thumbnail((RESOLUTION, RESOLUTION), Image.Resampling.LANCZOS)\n",
    "                \n",
    "                # Criar imagem quadrada com padding\n",
    "                new_img = Image.new('RGB', (RESOLUTION, RESOLUTION), (255, 255, 255))\n",
    "                paste_x = (RESOLUTION - img.width) // 2\n",
    "                paste_y = (RESOLUTION - img.height) // 2\n",
    "                new_img.paste(img, (paste_x, paste_y))\n",
    "                \n",
    "                new_img.save(new_path, 'PNG', quality=95)\n",
    "        \n",
    "        processed_count += 1\n",
    "        print(f\"✅ Processada: {os.path.basename(img_path)} -> {os.path.basename(new_path)}\")\n",
    "    \n",
    "    print(f\"\\n📊 Dataset de identidade preparado: {len(extracted_files)} imagens\")\n",
    "    print(f\"🎯 Imagens processadas: {processed_count}\")\n",
    "    !ls -la {instance_images_path}\n",
    "    \n",
    "    # Verificar se existe metadata do dataset\n",
    "    metadata_path = f\"{COLAB_DATASET_PATH}/dataset_metadata.json\"\n",
    "    if os.path.exists(metadata_path):\n",
    "        print(f\"📋 Metadata do dataset encontrado: {metadata_path}\")\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            import json\n",
    "            metadata = json.load(f)\n",
    "            if isinstance(metadata, list) and len(metadata) > 0:\n",
    "                first_meta = metadata[0]\n",
    "                print(f\"🧬 Configuração do dataset:\")\n",
    "                print(f\"   • Arquitetura: {first_meta.get('architecture', 'N/A')}\")\n",
    "                print(f\"   • Foco: {first_meta.get('optimization_focus', 'N/A')}\")\n",
    "                print(f\"   • Base Model: {first_meta.get('base_model', 'N/A')}\")\n",
    "                print(f\"   • Seeds: {metadata[0].get('seed', 'N/A')} a {metadata[-1].get('seed', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"❌ ERRO: {DATASET_ZIP_FILENAME} não encontrado!\")\n",
    "    print(\"🔍 Certifique-se de que:\")\n",
    "    print(\"1. Gerou o dataset usando valentina_dataset_generator_colab.ipynb\")\n",
    "    print(\"2. Baixou o arquivo valentina_identity_4lora_dataset_flux.zip\")\n",
    "    print(\"3. Está fazendo upload do arquivo correto\")\n",
    "    raise FileNotFoundError(\"Dataset de identidade não foi enviado\")\n",
    "\n",
    "print(\"\\n✅ Dataset de identidade preparado com sucesso!\")\n",
    "print(\"🧬 Pronto para treinar LoRA focada em identidade facial\")\n",
    "print(f\"🎭 Trigger word que será treinada: '{INSTANCE_PROMPT}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249e809",
   "metadata": {},
   "source": [
    "## Célula 6: Preparação do Modelo Base para Treinamento de Identidade\n",
    "Carrega o modelo FLUX.1-dev oficial e o prepara para treinamento da LoRA de identidade facial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2545649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"🚀 Iniciando preparação do modelo base para identidade...\")\n",
    "\n",
    "# === VALIDAÇÃO LOCAL DO DATASET (antes do upload) ===\n",
    "# 🔍 Verificação se o dataset foi criado corretamente ANTES do upload\n",
    "local_dataset_path = Path(\"valentina_identity_4lora_dataset_flux\")\n",
    "if local_dataset_path.exists():\n",
    "    local_images = list(local_dataset_path.glob(\"*.png\")) + list(local_dataset_path.glob(\"*.jpg\"))\n",
    "    if len(local_images) >= 15:\n",
    "        print(f\"✅ Dataset local válido: {len(local_images)} imagens encontradas\")\n",
    "        print(\"📦 Pronto para fazer upload do dataset para o Colab\")\n",
    "    else:\n",
    "        print(f\"⚠️ Dataset incompleto: apenas {len(local_images)} imagens (mínimo: 15)\")\n",
    "        print(\"🔄 Execute novamente o valentina_dataset_generator_colab.ipynb\")\n",
    "else:\n",
    "    print(\"❌ Pasta valentina_identity_4lora_dataset_flux não encontrada\")\n",
    "    print(\"🔄 Execute o valentina_dataset_generator_colab.ipynb primeiro\")\n",
    "\n",
    "# Configurar dtype baseado na GPU\n",
    "if torch.cuda.is_available():\n",
    "    if torch.cuda.get_device_capability()[0] >= 8:  # Ampere+\n",
    "        model_dtype = torch.bfloat16\n",
    "        print(\"💎 Usando bfloat16 (GPU Ampere+)\")\n",
    "    else:\n",
    "        model_dtype = torch.float16\n",
    "        print(\"⚡ Usando float16 (GPU older)\")\n",
    "else:\n",
    "    model_dtype = torch.float32\n",
    "    print(\"🐌 Usando float32 (CPU)\")\n",
    "\n",
    "# Para o treinamento, o mais simples é usar o modelo ID diretamente\n",
    "# O script de treinamento irá carregar o modelo automaticamente\n",
    "model_path = BASE_MODEL_ID\n",
    "print(f\"🎯 Modelo configurado para treinamento: {model_path}\")\n",
    "print(\"🧬 Foco: Treinamento de identidade facial pura\")\n",
    "\n",
    "# Verificar disponibilidade do modelo\n",
    "print(f\"✅ Modelo base preparado: {BASE_MODEL_ID}\")\n",
    "print(\"📝 O script de treinamento carregará o modelo automaticamente\")\n",
    "\n",
    "# Alternativamente, se precisar baixar o modelo localmente:\n",
    "try:\n",
    "    print(f\"\\n📦 Verificando disponibilidade do modelo: {BASE_MODEL_ID}\")\n",
    "    \n",
    "    # Teste rápido de carregamento para verificar acesso\n",
    "    pipeline = FluxPipeline.from_pretrained(\n",
    "        BASE_MODEL_ID,\n",
    "        torch_dtype=model_dtype,\n",
    "        use_safetensors=True,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    print(\"✅ Modelo acessível e compatível!\")\n",
    "    \n",
    "    # Salvar localmente se necessário\n",
    "    model_save_path = f\"{COLAB_MODELS_PATH}/flux_base_for_identity\"\n",
    "    print(f\"💾 Salvando modelo base em: {model_save_path}\")\n",
    "    \n",
    "    pipeline.save_pretrained(\n",
    "        model_save_path,\n",
    "        safe_serialization=True\n",
    "    )\n",
    "    \n",
    "    # Atualizar caminho para apontar para o modelo local\n",
    "    model_path = model_save_path\n",
    "    print(f\"🎯 Modelo salvo localmente: {model_path}\")\n",
    "    \n",
    "    # Verificar componentes do modelo\n",
    "    print(\"\\n🔍 Componentes do modelo verificados:\")\n",
    "    if hasattr(pipeline, 'transformer'):\n",
    "        print(\"   ✅ Transformer (componente principal)\")\n",
    "    if hasattr(pipeline, 'text_encoder'):\n",
    "        print(\"   ✅ Text Encoder\") \n",
    "    if hasattr(pipeline, 'text_encoder_2'):\n",
    "        print(\"   ✅ Text Encoder 2\")\n",
    "    if hasattr(pipeline, 'vae'):\n",
    "        print(\"   ✅ VAE\")\n",
    "    \n",
    "    print(f\"\\n🎯 Modelo preparado: {model_path}\")\n",
    "    print(\"🧬 Foco: Treinamento de identidade facial pura (sem LoRAs adicionais)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Não foi possível baixar o modelo localmente: {e}\")\n",
    "    print(\"🔄 Usando modelo ID diretamente para o treinamento\")\n",
    "    model_path = BASE_MODEL_ID\n",
    "    print(f\"🎯 Modelo para treinamento: {model_path}\")\n",
    "\n",
    "finally:\n",
    "    # Limpar memória\n",
    "    if 'pipeline' in locals():\n",
    "        del pipeline\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"🧹 Memória limpa após verificação do modelo\")\n",
    "\n",
    "print(f\"\\n✅ Preparação concluída!\")\n",
    "print(f\"🎭 Modelo pronto para treinamento de identidade: {model_path}\")\n",
    "print(\"🧬 Próximo: Iniciar treinamento da LoRA de identidade facial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd0a718",
   "metadata": {},
   "source": [
    "## Célula 7: Treinamento da LoRA de Identidade Facial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 CÉLULA 7: TREINAMENTO FLUX LORA DE IDENTIDADE FACIAL\n",
    "# Sistema completo otimizado para Google Colab\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from transformers import T5EncoderModel, T5Tokenizer, CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import FluxPipeline, FluxTransformer2DModel\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor2_0\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.optimization import get_scheduler\n",
    "import safetensors.torch as st\n",
    "\n",
    "print(\"🚀 INICIANDO TREINAMENTO FLUX LORA DE IDENTIDADE FACIAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configurar seeds para reprodutibilidade\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# Verificações iniciais\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8 else torch.float16\n",
    "\n",
    "print(f\"📱 Device: {device}\")\n",
    "print(f\"🔢 Dtype: {dtype}\")\n",
    "print(f\"💾 VRAM disponível: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "\n",
    "# Dataset personalizado para FLUX\n",
    "class FluxLoRADataset(Dataset):\n",
    "    def __init__(self, dataset_path, instance_prompt, class_prompt, tokenizer_1, tokenizer_2, size=1024):\n",
    "        self.dataset_path = Path(dataset_path)\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self.class_prompt = class_prompt\n",
    "        self.tokenizer_1 = tokenizer_1\n",
    "        self.tokenizer_2 = tokenizer_2\n",
    "        self.size = size\n",
    "        \n",
    "        # Encontrar imagens de instância\n",
    "        instance_path = self.dataset_path / \"instance_images\"\n",
    "        if not instance_path.exists():\n",
    "            instance_path = self.dataset_path\n",
    "        \n",
    "        self.instance_images = []\n",
    "        for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\"]:\n",
    "            self.instance_images.extend(list(instance_path.glob(ext)))\n",
    "        \n",
    "        print(f\"📊 Imagens de instância encontradas: {len(self.instance_images)}\")\n",
    "        \n",
    "        if len(self.instance_images) == 0:\n",
    "            raise ValueError(\"Nenhuma imagem de instância encontrada!\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instance_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Carregar e processar imagem\n",
    "        image_path = self.instance_images[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Redimensionar mantendo aspect ratio\n",
    "        image = image.resize((self.size, self.size), Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Converter para tensor\n",
    "        image = torch.from_numpy(np.array(image)).float() / 255.0\n",
    "        image = image.permute(2, 0, 1)  # HWC -> CHW\n",
    "        \n",
    "        # Normalizar para [-1, 1]\n",
    "        image = (image - 0.5) * 2.0\n",
    "        \n",
    "        # Tokenizar prompts\n",
    "        instance_tokens_1 = self.tokenizer_1(\n",
    "            self.instance_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        instance_tokens_2 = self.tokenizer_2(\n",
    "            self.instance_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        class_tokens_1 = self.tokenizer_1(\n",
    "            self.class_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        class_tokens_2 = self.tokenizer_2(\n",
    "            self.class_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids[0]\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"instance_prompt_ids_1\": instance_tokens_1,\n",
    "            \"instance_prompt_ids_2\": instance_tokens_2,\n",
    "            \"class_prompt_ids_1\": class_tokens_1,\n",
    "            \"class_prompt_ids_2\": class_tokens_2\n",
    "        }\n",
    "\n",
    "# Função para aplicar LoRA no transformer\n",
    "def setup_lora_layers(transformer):\n",
    "    \"\"\"Aplicar LoRA nos módulos de atenção do transformer FLUX (compatível FLUX.1-dev Colab)\"\"\"\n",
    "    lora_attn_procs = {}\n",
    "    for name, module in transformer.named_modules():\n",
    "        if \"attn\" in name and hasattr(module, \"to_k\"):\n",
    "            # Use only the processor key as required by set_attn_processor\n",
    "            lora_attn_procs[f\"{name}.processor\"] = LoRAAttnProcessor2_0()\n",
    "    transformer.set_attn_processor(lora_attn_procs)\n",
    "    return lora_attn_procs\n",
    "\n",
    "# Função principal de treinamento\n",
    "def train_flux_lora():\n",
    "    \"\"\"Executar treinamento FLUX LoRA completo\"\"\"\n",
    "    try:\n",
    "        print(\"\\n📥 FASE 1: Carregando modelo base...\")\n",
    "        \n",
    "        # Carregar pipeline FLUX\n",
    "        pipe = FluxPipeline.from_pretrained(\n",
    "            BASE_MODEL_ID,\n",
    "            torch_dtype=dtype,\n",
    "            use_safetensors=True,\n",
    "            variant=\"fp16\" if dtype == torch.float16 else None\n",
    "        )\n",
    "        \n",
    "        pipe = pipe.to(device)\n",
    "        print(f\"✅ Pipeline FLUX carregado: {BASE_MODEL_ID}\")\n",
    "        \n",
    "        # Extrair componentes\n",
    "        transformer = pipe.transformer\n",
    "        vae = pipe.vae\n",
    "        text_encoder = pipe.text_encoder\n",
    "        text_encoder_2 = pipe.text_encoder_2\n",
    "        tokenizer_1 = pipe.tokenizer\n",
    "        tokenizer_2 = pipe.tokenizer_2\n",
    "        scheduler = pipe.scheduler\n",
    "        \n",
    "        print(\"✅ Componentes extraídos com sucesso\")\n",
    "        \n",
    "        print(\"\\n📥 FASE 2: Configurando LoRA...\")\n",
    "        \n",
    "        # Configurar LoRA\n",
    "        lora_attn_procs = setup_lora_layers(transformer)\n",
    "        # Debug: inspecionar atributos dos processadores\n",
    "        print('--- DEBUG: Inspecionando attn_processors ---')\n",
    "        for name, proc in getattr(transformer, 'attn_processors', {}).items():\n",
    "            print(f'Processador: {name} | Tipo: {type(proc)}')\n",
    "            print('Atributos:', dir(proc))\n",
    "        print('--- FIM DEBUG ---')\n",
    "        # Obter parâmetros treináveis corretamente dos LoRAAttnProcessor2_0\n",
    "        trainable_params = []\n",
    "        for proc in getattr(transformer, 'attn_processors', {}).values():\n",
    "            for attr in ['to_q_lora', 'to_k_lora', 'to_v_lora', 'to_out_lora']:\n",
    "                lora_layer = getattr(proc, attr, None)\n",
    "                if lora_layer is not None:\n",
    "                    for p in lora_layer.parameters():\n",
    "                        if p.requires_grad:\n",
    "                            trainable_params.append(p)\n",
    "        print(f\"✅ LoRA configurado: {len(trainable_params)} parâmetros treináveis\")\n",
    "        print(f\"🔧 Rank: {LORA_RANK}, Alpha: {LORA_ALPHA}, Dropout: {LORA_DROPOUT}\")\n",
    "        \n",
    "        print(\"\\n📥 FASE 3: Preparando dataset...\")\n",
    "        \n",
    "        # Criar dataset\n",
    "        dataset = FluxLoRADataset(\n",
    "            COLAB_DATASET_PATH,\n",
    "            INSTANCE_PROMPT,\n",
    "            CLASS_PROMPT,\n",
    "            tokenizer_1,\n",
    "            tokenizer_2,\n",
    "            size=RESOLUTION\n",
    "        )\n",
    "        \n",
    "        # Criar dataloader\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=TRAIN_BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Dataset preparado: {len(dataset)} imagens\")\n",
    "        \n",
    "        print(\"\\n📥 FASE 4: Configurando otimizador...\")\n",
    "        \n",
    "        # Configurar otimizador\n",
    "        if USE_8BIT_ADAM:\n",
    "            try:\n",
    "                import bitsandbytes as bnb\n",
    "                optimizer_cls = bnb.optim.AdamW8bit\n",
    "                print(\"✅ Usando AdamW8bit\")\n",
    "            except ImportError:\n",
    "                optimizer_cls = torch.optim.AdamW\n",
    "                print(\"⚠️ Fallback para AdamW padrão\")\n",
    "        else:\n",
    "            optimizer_cls = torch.optim.AdamW\n",
    "        \n",
    "        optimizer = optimizer_cls(\n",
    "            trainable_params,\n",
    "            lr=LEARNING_RATE,\n",
    "            betas=(ADAM_BETA1, ADAM_BETA2),\n",
    "            weight_decay=ADAM_WEIGHT_DECAY,\n",
    "            eps=ADAM_EPSILON\n",
    "        )\n",
    "        \n",
    "        # Configurar scheduler\n",
    "        lr_scheduler = get_scheduler(\n",
    "            LR_SCHEDULER,\n",
    "            optimizer=optimizer,\n",
    "            num_warmup_steps=LR_WARMUP_STEPS,\n",
    "            num_training_steps=MAX_TRAIN_STEPS,\n",
    "            num_cycles=LR_NUM_CYCLES\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ Otimizador configurado: {optimizer_cls.__name__}\")\n",
    "        print(f\"📈 Scheduler: {LR_SCHEDULER}\")\n",
    "        \n",
    "        print(\"\\n📥 FASE 5: Iniciando treinamento...\")\n",
    "        \n",
    "        # Preparar modelos para treinamento\n",
    "        transformer.train()\n",
    "        vae.eval()\n",
    "        text_encoder.eval()\n",
    "        text_encoder_2.eval()\n",
    "        \n",
    "        # Desabilitar gradientes nos modelos não treináveis\n",
    "        vae.requires_grad_(False)\n",
    "        text_encoder.requires_grad_(False)\n",
    "        text_encoder_2.requires_grad_(False)\n",
    "        \n",
    "        # Variáveis de controle\n",
    "        global_step = 0\n",
    "        training_stats = []\n",
    "        \n",
    "        # Progress bar\n",
    "        progress_bar = tqdm(total=MAX_TRAIN_STEPS, desc=\"Treinamento\")\n",
    "        \n",
    "        # Loop de treinamento\n",
    "        epoch = 0\n",
    "        while global_step < MAX_TRAIN_STEPS:\n",
    "            epoch += 1\n",
    "            epoch_loss = 0\n",
    "            \n",
    "            for batch_idx, batch in enumerate(dataloader):\n",
    "                if global_step >= MAX_TRAIN_STEPS:\n",
    "                    break\n",
    "                \n",
    "                # Mover batch para device\n",
    "                pixel_values = batch[\"pixel_values\"].to(device, dtype=dtype)\n",
    "                instance_prompt_ids_1 = batch[\"instance_prompt_ids_1\"].to(device)\n",
    "                instance_prompt_ids_2 = batch[\"instance_prompt_ids_2\"].to(device)\n",
    "                class_prompt_ids_1 = batch[\"class_prompt_ids_1\"].to(device)\n",
    "                class_prompt_ids_2 = batch[\"class_prompt_ids_2\"].to(device)\n",
    "                \n",
    "                # Encode imagens com VAE\n",
    "                with torch.no_grad():\n",
    "                    latents = vae.encode(pixel_values).latent_dist.sample()\n",
    "                    latents = latents * vae.config.scaling_factor\n",
    "                \n",
    "                # Adicionar ruído\n",
    "                noise = torch.randn_like(latents)\n",
    "                timesteps = torch.randint(\n",
    "                    0, scheduler.config.num_train_timesteps,\n",
    "                    (latents.shape[0],), device=device\n",
    "                ).long()\n",
    "                \n",
    "                noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                # Encode prompts\n",
    "                with torch.no_grad():\n",
    "                    # Text encoder 1 (CLIP)\n",
    "                    encoder_hidden_states_1 = text_encoder(instance_prompt_ids_1)[0]\n",
    "                    class_hidden_states_1 = text_encoder(class_prompt_ids_1)[0]\n",
    "                    \n",
    "                    # Text encoder 2 (T5)\n",
    "                    encoder_hidden_states_2 = text_encoder_2(instance_prompt_ids_2)[0]\n",
    "                    class_hidden_states_2 = text_encoder_2(class_prompt_ids_2)[0]\n",
    "                \n",
    "                # Prediction do transformer\n",
    "                model_pred = transformer(\n",
    "                    noisy_latents,\n",
    "                    timesteps,\n",
    "                    encoder_hidden_states=encoder_hidden_states_1,\n",
    "                    pooled_projections=encoder_hidden_states_2.mean(dim=1),\n",
    "                    return_dict=False\n",
    "                )[0]\n",
    "                \n",
    "                # Calcular loss\n",
    "                if scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = scheduler.get_velocity(latents, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(f\"Prediction type {scheduler.config.prediction_type} não suportado\")\n",
    "                \n",
    "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "                \n",
    "                # Prior preservation loss\n",
    "                if PRIOR_LOSS_WEIGHT > 0:\n",
    "                    with torch.no_grad():\n",
    "                        class_model_pred = transformer(\n",
    "                            noisy_latents,\n",
    "                            timesteps,\n",
    "                            encoder_hidden_states=class_hidden_states_1,\n",
    "                            pooled_projections=class_hidden_states_2.mean(dim=1),\n",
    "                            return_dict=False\n",
    "                        )[0]\n",
    "                    \n",
    "                    prior_loss = F.mse_loss(class_model_pred.float(), target.float(), reduction=\"mean\")\n",
    "                    loss = loss + PRIOR_LOSS_WEIGHT * prior_loss\n",
    "                \n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "                \n",
    "                if (global_step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                    # Gradient clipping\n",
    "                    if MAX_GRAD_NORM > 0:\n",
    "                        torch.nn.utils.clip_grad_norm_(trainable_params, MAX_GRAD_NORM)\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                # Logging\n",
    "                current_loss = loss.detach().item()\n",
    "                epoch_loss += current_loss\n",
    "                \n",
    "                # Update progress\n",
    "                progress_bar.set_postfix({\n",
    "                    \"loss\": f\"{current_loss:.4f}\",\n",
    "                    \"lr\": f\"{lr_scheduler.get_last_lr()[0]:.2e}\",\n",
    "                    \"epoch\": epoch\n",
    "                })\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if global_step > 0 and global_step % SAVE_STEPS == 0:\n",
    "                    save_checkpoint(transformer, lora_layers, global_step)\n",
    "                \n",
    "                # Armazenar estatísticas\n",
    "                if global_step % 10 == 0:\n",
    "                    training_stats.append({\n",
    "                        \"step\": global_step,\n",
    "                        \"loss\": current_loss,\n",
    "                        \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "                        \"epoch\": epoch\n",
    "                    })\n",
    "                \n",
    "                global_step += 1\n",
    "                \n",
    "                # Limpeza de memória\n",
    "                if global_step % 50 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        progress_bar.close()\n",
    "        \n",
    "        print(\"\\n📥 FASE 6: Salvando modelo final...\")\n",
    "        \n",
    "        # Salvar LoRA final\n",
    "        save_final_lora(lora_layers, training_stats)\n",
    "        \n",
    "        print(\"\\n🎉 TREINAMENTO CONCLUÍDO COM SUCESSO!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERRO DURANTE TREINAMENTO: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "    \n",
    "    finally:\n",
    "        # Limpeza\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "def save_checkpoint(transformer, lora_layers, step):\n",
    "    \"\"\"Salvar checkpoint do treinamento\"\"\"\n",
    "    checkpoint_dir = f\"{COLAB_OUTPUT_PATH}/checkpoint-{step}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Salvar LoRA weights\n",
    "    lora_state_dict = lora_layers.state_dict()\n",
    "    checkpoint_path = f\"{checkpoint_dir}/pytorch_lora_weights.safetensors\"\n",
    "    st.save_file(lora_state_dict, checkpoint_path)\n",
    "    \n",
    "    print(f\"✅ Checkpoint salvo: {checkpoint_path}\")\n",
    "\n",
    "def save_final_lora(lora_layers, training_stats):\n",
    "    \"\"\"Salvar LoRA final e metadados\"\"\"\n",
    "    os.makedirs(COLAB_OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    # Salvar LoRA weights\n",
    "    lora_state_dict = lora_layers.state_dict()\n",
    "    lora_path = f\"{COLAB_OUTPUT_PATH}/valentina_identity_lora.safetensors\"\n",
    "    st.save_file(lora_state_dict, lora_path)\n",
    "    \n",
    "    # Salvar metadados\n",
    "    metadata = {\n",
    "        \"model_name\": \"Valentina Identity LoRA\",\n",
    "        \"base_model\": BASE_MODEL_ID,\n",
    "        \"training_config\": {\n",
    "            \"lora_rank\": LORA_RANK,\n",
    "            \"lora_alpha\": LORA_ALPHA,\n",
    "            \"lora_dropout\": LORA_DROPOUT,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"max_train_steps\": MAX_TRAIN_STEPS,\n",
    "            \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "            \"resolution\": RESOLUTION,\n",
    "            \"instance_prompt\": INSTANCE_PROMPT,\n",
    "            \"class_prompt\": CLASS_PROMPT\n",
    "        },\n",
    "        \"training_stats\": training_stats[-100:],  # Últimas 100 entradas\n",
    "        \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    with open(f\"{COLAB_OUTPUT_PATH}/training_metadata.json\", 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ LoRA salvo: {lora_path}\")\n",
    "    print(f\"📊 Tamanho: {os.path.getsize(lora_path) / 1024 / 1024:.1f}MB\")\n",
    "\n",
    "# Executar treinamento\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"🎯 Configurações de treinamento:\")\n",
    "    print(f\"   📊 Steps: {MAX_TRAIN_STEPS}\")\n",
    "    print(f\"   🧠 LoRA Rank: {LORA_RANK}\")\n",
    "    print(f\"   📈 Learning Rate: {LEARNING_RATE}\")\n",
    "    print(f\"   🎭 Trigger: '{INSTANCE_PROMPT}'\")\n",
    "    print(f\"   📐 Resolução: {RESOLUTION}x{RESOLUTION}\")\n",
    "    \n",
    "    success = train_flux_lora()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n✅ TREINAMENTO FINALIZADO COM SUCESSO!\")\n",
    "        print(\"🎉 LoRA de identidade facial da Valentina está pronta!\")\n",
    "    else:\n",
    "        print(\"\\n❌ TREINAMENTO FALHOU!\")\n",
    "        print(\"📋 Verifique os logs acima para detalhes do erro.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb51bbb",
   "metadata": {},
   "source": [
    "## Célula 8: Processamento dos Resultados de Identidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f411dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 CÉLULA 8: PROCESSAMENTO E DOWNLOAD DOS RESULTADOS\n",
    "# Análise, validação e empacotamento da LoRA de identidade facial\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from google.colab import files\n",
    "import torch\n",
    "import safetensors.torch as st\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import FluxPipeline\n",
    "\n",
    "print(\"📦 PROCESSAMENTO DOS RESULTADOS DE IDENTIDADE FACIAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def analyze_training_results():\n",
    "    \"\"\"Analisar resultados do treinamento\"\"\"\n",
    "    print(\"🔍 Analisando resultados do treinamento...\")\n",
    "    \n",
    "    results = {\n",
    "        \"lora_file\": None,\n",
    "        \"checkpoints\": [],\n",
    "        \"metadata\": None,\n",
    "        \"file_sizes\": {},\n",
    "        \"training_completed\": False\n",
    "    }\n",
    "    \n",
    "    # Verificar arquivo principal da LoRA\n",
    "    main_lora_path = f\"{COLAB_OUTPUT_PATH}/valentina_identity_lora.safetensors\"\n",
    "    if os.path.exists(main_lora_path):\n",
    "        results[\"lora_file\"] = main_lora_path\n",
    "        results[\"file_sizes\"][\"main_lora\"] = os.path.getsize(main_lora_path)\n",
    "        results[\"training_completed\"] = True\n",
    "        print(f\"✅ LoRA principal encontrada: {os.path.basename(main_lora_path)}\")\n",
    "        print(f\"   💾 Tamanho: {results['file_sizes']['main_lora'] / 1024 / 1024:.1f}MB\")\n",
    "    \n",
    "    # Verificar checkpoints\n",
    "    if os.path.exists(COLAB_OUTPUT_PATH):\n",
    "        for item in os.listdir(COLAB_OUTPUT_PATH):\n",
    "            if item.startswith(\"checkpoint-\"):\n",
    "                checkpoint_path = f\"{COLAB_OUTPUT_PATH}/{item}\"\n",
    "                lora_checkpoint = f\"{checkpoint_path}/pytorch_lora_weights.safetensors\"\n",
    "                if os.path.exists(lora_checkpoint):\n",
    "                    results[\"checkpoints\"].append({\n",
    "                        \"step\": int(item.split(\"-\")[1]),\n",
    "                        \"path\": lora_checkpoint,\n",
    "                        \"size\": os.path.getsize(lora_checkpoint)\n",
    "                    })\n",
    "    \n",
    "    # Ordenar checkpoints por step\n",
    "    results[\"checkpoints\"].sort(key=lambda x: x[\"step\"])\n",
    "    print(f\"📋 Checkpoints encontrados: {len(results['checkpoints'])}\")\n",
    "    \n",
    "    # Verificar metadados\n",
    "    metadata_path = f\"{COLAB_OUTPUT_PATH}/training_metadata.json\"\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            results[\"metadata\"] = json.load(f)\n",
    "        print(\"✅ Metadados de treinamento encontrados\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def validate_lora_weights(lora_path):\n",
    "    \"\"\"Validar integridade dos pesos da LoRA\"\"\"\n",
    "    print(f\"🔍 Validando LoRA: {os.path.basename(lora_path)}\")\n",
    "    \n",
    "    try:\n",
    "        # Carregar e analisar pesos\n",
    "        state_dict = st.load_file(lora_path)\n",
    "        \n",
    "        # Estatísticas dos pesos\n",
    "        total_params = 0\n",
    "        layer_info = {}\n",
    "        \n",
    "        for key, tensor in state_dict.items():\n",
    "            total_params += tensor.numel()\n",
    "            layer_type = key.split(\".\")[-2] if \".\" in key else \"unknown\"\n",
    "            \n",
    "            if layer_type not in layer_info:\n",
    "                layer_info[layer_type] = {\"count\": 0, \"params\": 0}\n",
    "            \n",
    "            layer_info[layer_type][\"count\"] += 1\n",
    "            layer_info[layer_type][\"params\"] += tensor.numel()\n",
    "        \n",
    "        print(f\"   📊 Parâmetros totais: {total_params:,}\")\n",
    "        print(f\"   🏗️ Camadas LoRA:\")\n",
    "        for layer_type, info in layer_info.items():\n",
    "            print(f\"      • {layer_type}: {info['count']} camadas, {info['params']:,} parâmetros\")\n",
    "        \n",
    "        # Verificar se há pesos não-zero\n",
    "        non_zero_weights = 0\n",
    "        total_weights = 0\n",
    "        \n",
    "        for tensor in state_dict.values():\n",
    "            non_zero_weights += (tensor != 0).sum().item()\n",
    "            total_weights += tensor.numel()\n",
    "        \n",
    "        non_zero_ratio = non_zero_weights / total_weights if total_weights > 0 else 0\n",
    "        print(f\"   🎨 Pesos não-zero: {non_zero_ratio:.1%}\")\n",
    "        \n",
    "        # Verificar range dos valores\n",
    "        all_values = torch.cat([tensor.flatten() for tensor in state_dict.values()])\n",
    "        print(f\"   📊 Range de valores: [{all_values.min():.4f}, {all_values.max():.4f}]\")\n",
    "        print(f\"   📊 Média: {all_values.mean():.4f}, Std: {all_values.std():.4f}\")\n",
    "        \n",
    "        validation_result = {\n",
    "            \"total_params\": total_params,\n",
    "            \"layer_info\": layer_info,\n",
    "            \"non_zero_ratio\": non_zero_ratio,\n",
    "            \"value_range\": [float(all_values.min()), float(all_values.max())],\n",
    "            \"mean\": float(all_values.mean()),\n",
    "            \"std\": float(all_values.std()),\n",
    "            \"valid\": True\n",
    "        }\n",
    "        \n",
    "        print(\"✅ LoRA validada com sucesso!\")\n",
    "        return validation_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro na validação: {e}\")\n",
    "        return {\"valid\": False, \"error\": str(e)}\n",
    "\n",
    "def create_test_generation(lora_path):\n",
    "    \"\"\"Criar geração de teste com a LoRA\"\"\"\n",
    "    print(\"🎨 Criando geração de teste...\")\n",
    "    \n",
    "    try:\n",
    "        # Carregar pipeline\n",
    "        pipe = FluxPipeline.from_pretrained(\n",
    "            BASE_MODEL_ID,\n",
    "            torch_dtype=torch.float16,\n",
    "            use_safetensors=True\n",
    "        )\n",
    "        \n",
    "        # Carregar LoRA\n",
    "        pipe.load_lora_weights(lora_path)\n",
    "        pipe = pipe.to(\"cuda\")\n",
    "        \n",
    "        # Prompt de teste\n",
    "        test_prompt = f\"a professional portrait photo of {INSTANCE_PROMPT.replace('a photo of ', '')}, high quality, detailed face, studio lighting\"\n",
    "        \n",
    "        print(f\"   📝 Prompt: {test_prompt}\")\n",
    "        \n",
    "        # Gerar imagem\n",
    "        with torch.no_grad():\n",
    "            image = pipe(\n",
    "                prompt=test_prompt,\n",
    "                height=1024,\n",
    "                width=1024,\n",
    "                num_inference_steps=25,\n",
    "                guidance_scale=7.5,\n",
    "                generator=torch.Generator(device=\"cuda\").manual_seed(42)\n",
    "            ).images[0]\n",
    "        \n",
    "        # Salvar imagem de teste\n",
    "        test_image_path = f\"{COLAB_OUTPUT_PATH}/test_generation.png\"\n",
    "        image.save(test_image_path)\n",
    "        \n",
    "        print(f\"✅ Geração de teste salva: {test_image_path}\")\n",
    "        \n",
    "        # Limpar memória\n",
    "        del pipe\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return test_image_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Erro na geração de teste: {e}\")\n",
    "        print(\"   📝 Isso pode ser normal se o Colab não tiver VRAM suficiente\")\n",
    "        return None\n",
    "\n",
    "def create_documentation(results, validation_data):\n",
    "    \"\"\"Criar documentação completa da LoRA\"\"\"\n",
    "    print(\"📝 Criando documentação...\")\n",
    "    \n",
    "    # Calcular estatísticas do dataset\n",
    "    instance_images_path = f\"{COLAB_DATASET_PATH}/instance_images\"\n",
    "    if not os.path.exists(instance_images_path):\n",
    "        instance_images_path = COLAB_DATASET_PATH\n",
    "    \n",
    "    image_files = []\n",
    "    for ext in [\"*.jpg\", \"*.jpeg\", \"*.png\"]:\n",
    "        image_files.extend(list(Path(instance_images_path).glob(ext)))\n",
    "    \n",
    "    # Criar README detalhado\n",
    "    readme_content = f\"\"\"# Valentina Identity LoRA - FLUX.1-dev\n",
    "\n",
    "## 🎯 Objetivo\n",
    "Esta LoRA foi treinada especificamente para capturar e preservar a **identidade facial da Valentina**, focando em:\n",
    "- Consistência de características faciais\n",
    "- Preservação da identidade visual\n",
    "- Geração de alta qualidade com FLUX.1-dev\n",
    "\n",
    "## 📋 Informações Técnicas\n",
    "\n",
    "### Modelo Base\n",
    "- **Arquitetura**: FLUX.1-dev\n",
    "- **Desenvolvedor**: Black Forest Labs\n",
    "- **Tipo**: Modelo de difusão transformer\n",
    "\n",
    "### Configurações de Treinamento\n",
    "- **LoRA Rank**: {LORA_RANK}\n",
    "- **LoRA Alpha**: {LORA_ALPHA}\n",
    "- **LoRA Dropout**: {LORA_DROPOUT}\n",
    "- **Learning Rate**: {LEARNING_RATE}\n",
    "- **Steps de Treinamento**: {MAX_TRAIN_STEPS}\n",
    "- **Batch Size**: {TRAIN_BATCH_SIZE}\n",
    "- **Resolução**: {RESOLUTION}x{RESOLUTION}\n",
    "- **Imagens de Treinamento**: {len(image_files)}\n",
    "\n",
    "### Dataset\n",
    "- **Fonte**: Gerado pelo valentina_dataset_generator_colab.ipynb\n",
    "- **Tipo**: Imagens com seeds sequenciais para máxima consistência\n",
    "- **Foco**: Identidade facial pura (não NSFW)\n",
    "- **Características preservadas**:\n",
    "  - Idade: 25 anos\n",
    "  - Formato do rosto: Oval\n",
    "  - Olhos: Amendoados castanho-escuros\n",
    "  - Lábios: Carnudos com arco do cupido\n",
    "  - Pele: Dourada mediterrânea\n",
    "  - Cabelo: Castanho médio com reflexos dourados\n",
    "  - Tatuagens: NENHUMA (pele limpa)\n",
    "\n",
    "## 🚀 Como Usar\n",
    "\n",
    "### mflux (macOS)\n",
    "```bash\n",
    "mflux-generate \\\\\n",
    "    --model \"/path/to/flux.1-dev\" \\\\\n",
    "    --lora \"valentina_identity_lora.safetensors\" \\\\\n",
    "    --lora-scale 0.8 \\\\\n",
    "    --prompt \"a photo of vltna woman, [seu prompt]\" \\\\\n",
    "    --steps 25 \\\\\n",
    "    --height 1024 \\\\\n",
    "    --width 1024\n",
    "```\n",
    "\n",
    "### ComfyUI\n",
    "1. Coloque o arquivo .safetensors na pasta `models/loras/`\n",
    "2. Use o nó \"Load LoRA\" no workflow\n",
    "3. Configure o peso entre 0.7-1.0\n",
    "4. Use o trigger word `vltna woman` nos prompts\n",
    "\n",
    "### Automatic1111\n",
    "1. Coloque o arquivo na pasta `models/Lora/`\n",
    "2. Use `<lora:valentina_identity_lora:0.8>` no prompt\n",
    "3. Inclua `vltna woman` no prompt\n",
    "\n",
    "## 🎨 Prompts Recomendados\n",
    "\n",
    "### Para Identidade/Retratos\n",
    "```\n",
    "a photo of vltna woman, professional portrait, studio lighting, detailed face\n",
    "vltna woman, natural smile, elegant pose, high quality photography\n",
    "a photo of vltna woman, looking at camera, soft lighting, photorealistic\n",
    "```\n",
    "\n",
    "### Para Situações Específicas\n",
    "```\n",
    "vltna woman, business attire, office environment, confident expression\n",
    "a photo of vltna woman, casual outfit, outdoor scene, natural lighting\n",
    "vltna woman, elegant dress, formal event, sophisticated pose\n",
    "```\n",
    "\n",
    "## ⚙️ Configurações Recomendadas\n",
    "\n",
    "| Parâmetro | Valor Recomendado | Observações |\n",
    "|-----------|------------------|-------------|\n",
    "| LoRA Weight | 0.7 - 1.0 | Comece com 0.8 |\n",
    "| Steps | 25 - 35 | 25 é ideal para velocidade |\n",
    "| CFG Scale | 7 - 9 | 7.5 é um bom ponto de partida |\n",
    "| Resolução | 1024x1024 | Resolução de treinamento |\n",
    "| Sampler | DPM++ 2M | Para FLUX.1-dev |\n",
    "\n",
    "## 📋 Resultados de Validação\n",
    "\n",
    "- **Parâmetros Totais**: {validation_data.get('total_params', 'N/A'):,}\n",
    "- **Pesos Não-Zero**: {validation_data.get('non_zero_ratio', 0):.1%}\n",
    "- **Range de Valores**: [{validation_data.get('value_range', [0, 0])[0]:.4f}, {validation_data.get('value_range', [0, 0])[1]:.4f}]\n",
    "- **Média dos Pesos**: {validation_data.get('mean', 0):.4f}\n",
    "\n",
    "## ⚠️ Notas Importantes\n",
    "\n",
    "1. **Foco em Identidade**: Esta LoRA foi treinada especificamente para preservar a identidade facial da Valentina\n",
    "2. **Não NSFW**: Otimizada para geração de conteúdo não explícito\n",
    "3. **Consistência**: Use seeds semelhantes para manter consistência entre gerações\n",
    "4. **Qualidade**: Melhor qualidade com resoluções altas (1024x1024 ou superior)\n",
    "5. **Trigger Word**: Sempre inclua `vltna woman` para ativar a identidade\n",
    "\n",
    "## 📊 Performance\n",
    "\n",
    "- **VRAM Necessária**: ~8-12GB para geração 1024x1024\n",
    "- **Tempo de Geração**: ~30-60 segundos (dependendo da GPU)\n",
    "- **Qualidade**: Alta fidelidade à identidade da Valentina\n",
    "- **Estabilidade**: Testada em múltiplas gerações\n",
    "\n",
    "## 🔄 Atualizações\n",
    "\n",
    "- **Versão**: 1.0 (Treinamento inicial de identidade)\n",
    "- **Data**: {results.get('metadata', {}).get('timestamp', 'N/A')}\n",
    "- **Status**: Pronta para uso em produção\n",
    "\n",
    "## 📞 Suporte\n",
    "\n",
    "Para problemas ou dúvidas:\n",
    "1. Verifique se está usando o modelo base correto (FLUX.1-dev)\n",
    "2. Confirme que o trigger word `vltna woman` está no prompt\n",
    "3. Ajuste o peso da LoRA entre 0.7-1.0\n",
    "4. Use as configurações recomendadas acima\n",
    "\n",
    "---\n",
    "\n",
    "**Gerado pelo Sistema de Treinamento Valentina LoRA**  \n",
    "**Dataset**: valentina_identity_4lora_dataset_flux  \n",
    "**Notebook**: valentina_colab_facial_lora_trainer_flux.ipynb\n",
    "\"\"\"\n",
    "    \n",
    "    return readme_content\n",
    "\n",
    "def create_final_package(results, validation_data):\n",
    "    \"\"\"Criar pacote final para download\"\"\"\n",
    "    print(\"📦 Criando pacote final...\")\n",
    "    \n",
    "    # Criar diretório do pacote\n",
    "    package_dir = f\"{COLAB_OUTPUT_PATH}/valentina_identity_lora_package\"\n",
    "    os.makedirs(package_dir, exist_ok=True)\n",
    "    \n",
    "    # Copiar LoRA principal\n",
    "    if results[\"lora_file\"]:\n",
    "        final_lora_name = \"valentina_identity_lora.safetensors\"\n",
    "        shutil.copy2(results[\"lora_file\"], f\"{package_dir}/{final_lora_name}\")\n",
    "        print(f\"✅ LoRA copiada: {final_lora_name}\")\n",
    "    \n",
    "    # Criar documentação\n",
    "    readme_content = create_documentation(results, validation_data)\n",
    "    with open(f\"{package_dir}/README.md\", 'w', encoding='utf-8') as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    # Criar arquivo de configuração JSON\n",
    "    config = {\n",
    "        \"model_info\": {\n",
    "            \"name\": \"Valentina Identity LoRA\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"base_model\": BASE_MODEL_ID,\n",
    "            \"type\": \"identity_lora\",\n",
    "            \"architecture\": \"flux_transformer\"\n",
    "        },\n",
    "        \"training_config\": {\n",
    "            \"lora_rank\": LORA_RANK,\n",
    "            \"lora_alpha\": LORA_ALPHA,\n",
    "            \"lora_dropout\": LORA_DROPOUT,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"max_steps\": MAX_TRAIN_STEPS,\n",
    "            \"batch_size\": TRAIN_BATCH_SIZE,\n",
    "            \"resolution\": RESOLUTION,\n",
    "            \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS\n",
    "        },\n",
    "        \"character_profile\": {\n",
    "            \"name\": \"Valentina Moreau\",\n",
    "            \"age\": 25,\n",
    "            \"face_shape\": \"oval\",\n",
    "            \"eyes\": \"amendoados castanho-escuros\",\n",
    "            \"lips\": \"carnudos com arco do cupido\",\n",
    "            \"skin\": \"dourada mediterrânea\",\n",
    "            \"hair\": \"castanho médio com reflexos dourados\",\n",
    "            \"body_type\": \"curvilíneo\",\n",
    "            \"tattoos\": \"nenhuma - pele limpa\"\n",
    "        },\n",
    "        \"usage\": {\n",
    "            \"trigger_word\": INSTANCE_PROMPT,\n",
    "            \"recommended_weight\": \"0.7-1.0\",\n",
    "            \"compatible_software\": [\"mflux\", \"ComfyUI\", \"Automatic1111\"],\n",
    "            \"optimal_resolution\": \"1024x1024\",\n",
    "            \"recommended_steps\": \"25-35\"\n",
    "        },\n",
    "        \"validation\": validation_data,\n",
    "        \"created_at\": results.get(\"metadata\", {}).get(\"timestamp\", \"unknown\")\n",
    "    }\n",
    "    \n",
    "    with open(f\"{package_dir}/config.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Copiar imagem de teste se existir\n",
    "    test_image_path = f\"{COLAB_OUTPUT_PATH}/test_generation.png\"\n",
    "    if os.path.exists(test_image_path):\n",
    "        shutil.copy2(test_image_path, f\"{package_dir}/sample_generation.png\")\n",
    "        print(\"✅ Imagem de teste incluída\")\n",
    "    \n",
    "    # Copiar metadados de treinamento se existir\n",
    "    metadata_path = f\"{COLAB_OUTPUT_PATH}/training_metadata.json\"\n",
    "    if os.path.exists(metadata_path):\n",
    "        shutil.copy2(metadata_path, f\"{package_dir}/training_metadata.json\")\n",
    "    \n",
    "    # Criar arquivo ZIP\n",
    "    zip_filename = f\"{COLAB_OUTPUT_PATH}/valentina_identity_lora_final.zip\"\n",
    "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED, compresslevel=6) as zipf:\n",
    "        for root, dirs, files in os.walk(package_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arc_name = os.path.relpath(file_path, package_dir)\n",
    "                zipf.write(file_path, arc_name)\n",
    "    \n",
    "    package_size = os.path.getsize(zip_filename) / 1024 / 1024\n",
    "    print(f\"✅ Pacote criado: {zip_filename}\")\n",
    "    print(f\"💾 Tamanho do pacote: {package_size:.1f}MB\")\n",
    "    \n",
    "    return zip_filename, package_size\n",
    "\n",
    "def display_summary(results, validation_data, package_info):\n",
    "    \"\"\"Exibir resumo final\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎉 RESUMO FINAL DO TREINAMENTO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if results[\"training_completed\"]:\n",
    "        print(\"✅ STATUS: TREINAMENTO CONCLUÍDO COM SUCESSO\")\n",
    "    else:\n",
    "        print(\"⚠️ STATUS: TREINAMENTO INCOMPLETO OU COM PROBLEMAS\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n🎯 MODELO:\")\n",
    "    print(f\"   • Nome: Valentina Identity LoRA\")\n",
    "    print(f\"   • Base: {BASE_MODEL_ID}\")\n",
    "    print(f\"   • Tipo: Identidade Facial (não NSFW)\")\n",
    "    print(f\"   • Trigger: '{INSTANCE_PROMPT}'\")\n",
    "    \n",
    "    print(f\"\\n📊 ESTATÍSTICAS:\")\n",
    "    if validation_data.get('valid'):\n",
    "        print(f\"   • Parâmetros LoRA: {validation_data['total_params']:,}\")\n",
    "        print(f\"   • Pesos ativos: {validation_data['non_zero_ratio']:.1%}\")\n",
    "        print(f\"   • Tamanho do arquivo: {results['file_sizes']['main_lora'] / 1024 / 1024:.1f}MB\")\n",
    "    \n",
    "    print(f\"\\n🔧 CONFIGURAÇÕES:\")\n",
    "    print(f\"   • LoRA Rank: {LORA_RANK}\")\n",
    "    print(f\"   • LoRA Alpha: {LORA_ALPHA}\")\n",
    "    print(f\"   • Steps: {MAX_TRAIN_STEPS}\")\n",
    "    print(f\"   • Learning Rate: {LEARNING_RATE}\")\n",
    "    print(f\"   • Resolução: {RESOLUTION}x{RESOLUTION}\")\n",
    "    \n",
    "    if package_info:\n",
    "        zip_file, size = package_info\n",
    "        print(f\"\\n📦 PACOTE FINAL:\")\n",
    "        print(f\"   • Arquivo: {os.path.basename(zip_file)}\")\n",
    "        print(f\"   • Tamanho: {size:.1f}MB\")\n",
    "        print(f\"   • Conteúdo: LoRA + Documentação + Config\")\n",
    "    \n",
    "    print(f\"\\n🚀 COMO USAR:\")\n",
    "    print(f\"   1. Baixe o arquivo ZIP\")\n",
    "    print(f\"   2. Extraia e use o arquivo .safetensors\")\n",
    "    print(f\"   3. Configure peso 0.7-1.0 no seu software\")\n",
    "    print(f\"   4. Use '{INSTANCE_PROMPT}' nos prompts\")\n",
    "    print(f\"   5. Leia o README.md para detalhes\")\n",
    "    \n",
    "    print(f\"\\n🎨 CARACTERÍSTICAS PRESERVADAS:\")\n",
    "    print(f\"   • Idade: 25 anos\")\n",
    "    print(f\"   • Rosto: Oval\")\n",
    "    print(f\"   • Olhos: Amendoados castanho-escuros\")\n",
    "    print(f\"   • Pele: Dourada mediterrânea, sem tatuagens\")\n",
    "    print(f\"   • Cabelo: Castanho médio com reflexos\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Execução principal\n",
    "if __name__ == \"__main__\":\n",
    "    # Analisar resultados\n",
    "    results = analyze_training_results()\n",
    "    \n",
    "    if not results[\"training_completed\"]:\n",
    "        print(\"❌ ERRO: Treinamento não foi concluído ou LoRA não foi encontrada!\")\n",
    "        print(\"\\n🔍 Verificando diretório de saída:\")\n",
    "        if os.path.exists(COLAB_OUTPUT_PATH):\n",
    "            for item in os.listdir(COLAB_OUTPUT_PATH):\n",
    "                item_path = os.path.join(COLAB_OUTPUT_PATH, item)\n",
    "                if os.path.isfile(item_path):\n",
    "                    size = os.path.getsize(item_path) / 1024 / 1024\n",
    "                    print(f\"   📄 {item} ({size:.1f}MB)\")\n",
    "                else:\n",
    "                    print(f\"   📁 {item}/\")\n",
    "        else:\n",
    "            print(f\"   ❌ Diretório não existe: {COLAB_OUTPUT_PATH}\")\n",
    "    else:\n",
    "        # Validar LoRA\n",
    "        validation_data = validate_lora_weights(results[\"lora_file\"])\n",
    "        \n",
    "        # Criar geração de teste (opcional)\n",
    "        test_image = create_test_generation(results[\"lora_file\"])\n",
    "        \n",
    "        # Criar pacote final\n",
    "        package_info = create_final_package(results, validation_data)\n",
    "        \n",
    "        # Exibir resumo\n",
    "        display_summary(results, validation_data, package_info)\n",
    "        \n",
    "        # Download do pacote\n",
    "        if package_info:\n",
    "            zip_file, _ = package_info\n",
    "            print(f\"\\n📥 INICIANDO DOWNLOAD...\")\n",
    "            files.download(zip_file)\n",
    "            print(f\"✅ Download concluído: {os.path.basename(zip_file)}\")\n",
    "            \n",
    "            print(f\"\\n🎉 PARABÉNS!\")\n",
    "            print(f\"Sua LoRA de identidade facial da Valentina está pronta para uso!\")\n",
    "            print(f\"\\nPróximos passos:\")\n",
    "            print(f\"1. Extraia o arquivo ZIP baixado\")\n",
    "            print(f\"2. Leia o README.md para instruções detalhadas\")\n",
    "            print(f\"3. Teste com o prompt: 'a photo of vltna woman, portrait'\")\n",
    "            print(f\"4. Ajuste o peso da LoRA conforme necessário (0.7-1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28326a4e",
   "metadata": {},
   "source": [
    "## Célula 9: Limpeza (Opcional)\n",
    "Descomente para remover arquivos grandes e economizar espaço no Colab se for continuar usando o runtime após o treinamento de identidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f23b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Limpando arquivos de treinamento de identidade...\")\n",
    "# !rm -rf {COLAB_MODELS_PATH}/{BASE_MODEL_ID.split('/')[-1]} # Remove o cache do modelo base\n",
    "# !rm -f {DATASET_ZIP_FILENAME} # Remove o dataset ZIP\n",
    "# !rm -rf {COLAB_DATASET_PATH}/* # Limpa imagens extraídas do dataset de identidade\n",
    "# print(\"Limpeza concluída (arquivos de modelo e dataset de identidade).\")\n",
    "# torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
